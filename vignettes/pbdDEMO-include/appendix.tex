\chapter{Numerical Linear Algebra and Linear Least Squares Problems}
\label{apx:numlinalg}

\inspire{Mathematics is written for mathematicians.}{Nicolaus Copernicus}
\vspace{0.5cm}


For the remainder, assume that all matrices are real-valued.

Let us revisit the problem of solving linear least squares problems, introduced in Section~\ref{sec:ols}.  Recall that we wish to find a solution $\bbeta$ such that
\begin{align*}
\ltwo{ \bX\bbeta - \by }^2
\end{align*}
In the case that $\bX$ is full rank (which is often assumed, whether reasonable or not), this has analytical solution
\begin{align}
 \hat{\bbeta} = (\bX^T\bX)^{-1}\bX^T\by \label{math:ols2}
\end{align}
However, even with this nice closed form, implementing this efficiently on a computer is not entirely straightforward.  Herein we discuss several of the issues in implementing the linear least squares solution efficiently.  For simplicity, we will assume that $\bX$ is full rank, although this is not necessary --- although rank degeneracy does complicate things.  For more details on the rank degeneracy problem, and linear least squares problems in general, see the classic \emph{Matrix Computations} \citep{gvl}.




\section{Forming the Normal Equations}
\label{asec:nrmleq}
If we wish to implement this numerically, then rather than directly computing the inverse of $\bX^T\bX$ directly, we would instead compute the Cholesky factorization
\begin{align*}
\bX^T\bX = LL^T
\end{align*}
where $L$ is lower triangular.  Then turning to the so-called ``normal equations''
\begin{align}
 \left(\bX^T\bX\right)\bbeta &= \bX^T\by\label{math:normeq}
\end{align}
by simple substitution and grouping, we have
\begin{align*}
L\left(L^T\bbeta\right) &= \bX^T\by
\end{align*}
Now, since $L$ is triangular, these two triangular systems (one forward and one backward substitution found by careful grouping of terms above) can be solved in a numerically stable way \citep{numericalstab}.  However, forming the Cholesky factorization itself suffers from the effects of roundoff error in having to form the product $\bX^T\bX$.  We elaborate on this to a degree in the following section.




\section{Using the QR Factorization}
\label{asec:qr}
Directly computing the normal equations is ill advised, because it is often impossible to do so with adequate numerical precision.  To fully appreciate this problem, we must entertain a brief discussion about condition numbers.

By definition, if a matrix has finite condition number, then it must have been invertible.  However, for numerical methods, a condition number which is ``big enough'' is essentially infinite (loosely speaking).  And observe that forming the product $\bX^T\bX$ squares the condition number of $\bX$:
\begin{align*}
\kappa\left(\bX^T\bX\right) &= \norm{\bX^T\bX}\norm{\left(\bX^T\bX\right)^{-1}}\\
  &=\norm{\bX^T\bX} \norm{\bX^{-1}\left(\bX^T\right)^{-1}}\\
  &=\norm{\bX^T} \norm{\bX} \norm{\bX^{-1}} \norm{\bX^{-T}}\\
  &=\norm{\bX} \norm{\bX} \norm{\bX^{-1}} \norm{\bX^{-1}}\\
  &= \norm{\bX}^2 \norm{\bX^{-1}}^2\\
  &=\kappa(\bX)^2
\end{align*}
So if $\kappa\left(\bX\right)$ is ``large'', then forming this product can lead to large numerical errors when attempting to numerically invert or factor a matrix, or solve a system of equations.

To avoid this problem, the orthogonal QR Decomposition is typically used.  Here we take 
\begin{align*}
\bX=QR
\end{align*}
where $Q$ is orthogonal and $R$ is upper trapezoidal (n the overdetermined case, $R$ is triangular).  This is beneficial, because orthogonal matrices are norm-preserving, i.e. $Q$ is an isometry, and whence
\begin{align*}
\ltwo{\bX\bbeta - \by} &= \ltwo{QR\bbeta - \by}\\
  &= \ltwo{Q^TQR\bbeta - Q^T\by}\\
  &= \ltwo{R\bbeta - Q^T\by}
\end{align*}
This amounts to solving the triangular system
\begin{align*}
R\bbeta = Q^T\by
\end{align*}
As noted in Section~\ref{asec:nrmleq}, solving this system can be done in a numerically stable fashion (and the high performance libraries employed by both \proglang{R} and
\pbdR use stable implementations).  The key difference here is that the QR 
factorization is of $\bX$, not $\bX^T\bX$, and so we need only worry about the 
conditioning of $\bX$ (as opposed to its squared condition number).

While this method is much less prone to the numerical issues discussed above, it is much slower computationally.  Additionally, we note that unlike the method in forming the normal equations, this method can be extended to the rank degenerate case.




\section{Using the Singular Value Decomposition}
There is another, arguably much more well-known matrix factorization which we can use to develop yet another analytically equivalent solution to the least squares problem, namely the singular value decomposition
(SVD).~\index{singular value decomposition}\index{Decomposition!SVD}  Using this factorization leads to a very elegant solution, as is so often the case with the SVD.  

Note that in (\ref{math:ols2}), the quantity
\begin{align*}
(\bX^T\bX)^{-1}\bX^T
\end{align*}
is the Moore-Penrose inverse of $\bX$.  So if we take
\begin{align*}
\bX = U\Sigma V^T
\end{align*}
to be the SVD of $\bX$, then we have
\begin{align*}
\bX^+ &= \left(\bX^T\bX\right)^{-1}\bX^T\\
  &= \left( \left(U\Sigma V^T\right)^T \left(U\Sigma V^T\right) \right)^{-1} U\Sigma V^T\\
  &= \left( V\Sigma^T\Sigma V^T \right)^{-1} V\Sigma^T U^T\\
  &= V \left( \left(\Sigma^T\Sigma\right)^{-1} \Sigma^T \right) U^T\\
  &= V \Sigma^+ U^T
\end{align*}
Whence,
\begin{align*}
\bbeta = V\Sigma^+ U^T\by
\end{align*}
Conceptually, this is arguably the most elegant method of solving the linear least squares problem.  Additionally, as with the QR method above, with slight modification the above argument can extend to the rank degenerate case; however, we suspect that the SVD is much more well known to mathematicians and statisticians than is the QR decomposition.  This abstraction comes at a great cost, though, as this approach is handily the most computationally intensive of the three presented here.




\chapter{Linear Regression and Rank Degeneracy in R}


\inspire{When a doctor does go wrong, he is the first of criminals. He has the nerve and he has the knowledge.}{Sherlock Holmes}
\vspace{0.5cm}


In the case that $\bX$ is rank deficient, then $\bX$ (and whence $\bX^T\bX$) is not invertible, so the problem can not be solved by the method of Section~\ref{asec:nrmleq}.  Both \proglang{R} and
\pbdR use a QR factorization as in Section~\ref{asec:qr}, although the two 
systems use a slightly different approach.  While most of the linear algebra in 
\proglang{R} is handled by
LAPACK~\citep{lug},~\index{Library!LAPACK} arguably the most important numerical function in all of \proglang{R}, namely \code{lm.fit()} used by \code{lm()} to fit linear regression models, uses LINPACK \citep{linpack}.  By comparison to LAPACK,
LINPACK~\index{Library!LINPACK} is much less sophisticated.  However,
\pbdR uses level 3
PBLAS~\index{Library!PBLAS} and ScaLAPACK (the distributed equivalent of using level 3 BLAS and LAPACK) to fit linear regression models.

The LINPACK routines used by \proglang{R} are \code{DQRLS}, which calls a modified \code{DQRDC2} to compute a rank-revealing
QR factorization\index{Decomposition!QR}
with a ``limited pivoting strategy'' (more on this later).  Finally, \code{DQRSL} is called to apply the output of the QR factorization to compute the least squares soluations.  By contrast,
\pbdR uses a modified \code{PDGELS} routine, which uses a version of 
\code{PDGEQPF} modified to use \proglang{R}'s ``limited pivoting strategy'', and 
then calls \code{PDORMQR} to fit the least squares solution.

Neither approach assumes that the model matrix is full rank.  Instead, the methods are \emph{rank-revealing}, in that they attempt to discover the numerical rank while computing the orthogonal factorization.  However, both \proglang{R} and (for the sake of consistency)
\pbdR use a ``limited pivoting strategy'' (with language, we believe, due to 
Ross Ihaka) in determining numerical rank.  Generally when computing a QR with 
pivoting, for the sake of numerical stability one chooses the column with 
largest partial norm while forming the Householder reflections.  However, in 
doing so it is possible to permute the columns in such a way that a desired 
statistical interpretation (such as in an ANOVA) is destroyed.  The solution 
employed by \proglang{R} is to merely iterate over the columns and choose the 
current column as the pivot each time.  When a column is detected to have 
``small'' partial norm, it is pushed to the back.  The author of these 
modification writes:
\begin{quote}
     a limited column pivoting strategy based on the 2-norms 
     of the reduced columns moves columns with near-zero norm to the 
     right-hand edge of the x matrix.  this strategy means that 
     sequential one degree-of-freedom effects can be computed in a 
     natural way.

     i am very nervous about modifying linpack code in this way.
     if you are a computational linear algebra guru and you really
     understand how to solve this problem please feel free to
     suggest improvements to this code.
\end{quote}
So in this way, if a model matrix is full rank, then the estimates coming from \proglang{R} should be considered at least as trustworthy as probably every other statistical software package of note.  If it is not, then this method presents a possible numerical stability issue; although to what degree, if any at all, this is actually a problem, the authors at present have no real knowledge.  If numerical precision is absolutely paramount, consider using the SVD to solve the least squares problem; though do be aware that this is hands down the slowest possible approach.

We again note that the limited pivoting strategy of \proglang{R} is employed by
\pbdR in the \code{lm.fit()} method for
class \code{ddmatrix}.\index{Class!\code{ddmatrix}}
