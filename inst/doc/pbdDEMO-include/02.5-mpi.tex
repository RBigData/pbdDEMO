
%%% ----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{MPI for the R User}
\label{chp:mpi_for_the_r_user}

Cicero once said that ``If you have a garden and a library, you have everything you need.''  So in that spirit, for the next two chapters we will use the MPI library to get our hands dirty and root around in the dirt of low-level MPI programming.

\section{MPI Basics}

In a sense, Cicero (in the above tortured metaphor) was quite right.  MPI is all that we \emph{need} in the same way that I might only \emph{need} bread and cheese, but really what I \emph{want} is a pizza.  MPI is somewhat low-level and can be quite fiddly, but mastering it adds a very powerful tool to the repertoire of the parallel \proglang{R} programmer, and is essential for anyone who wants to do large scale development of parallel codes.  

``MPI'' stands for ``Message Passing Interface''.  How it really works goes \emph{well} beyond the scope of this document.  But at a basic level, the idea is that the user is running a code on different compute nodes that (usually) can not directly modify objects in each others' memory.  In order to have all of the nodes working together on a common problem, data and computation directives are passed around over the network (often over a specialized link called infiniband).  

At its core, MPI is a standard interface for managing communications (data and instructions) between different nodes or computers.  There are several major implementations of this standard, and the one you should use may depend on the machine you are using.  	But this is a compiling issue, so user programs are unaffected beyond this minor hurdle.  Some of the most well-known implementations are OpenMPI, MPICH2, and Cray MPT. 

At the core of using MPI is the \emph{communicator}.  At a technical level, a communicator is a pretty complicated data structure, but these deep technical details are not necessary for proceeding.  We will instead think of it somewhat like the post office.  When we wish to send a letter (communication) to someone else (another processor), we merely drop the letter off at a post office mailbox (communicator) and trust that the post office (MPI) will handle things accordingly (sort of).  

The general process for directly --- or indirectly --- utilizing MPI in SPMD programs goes something like this:

\begin{enumerate}
 \item Initialize communicator(s).
 \item Have each process read in its portion of the data.
 \item Perform computations.
 \item Communicate results.
 \item Shut down the communicator(s).
\end{enumerate}

Some of the above steps may be swept away under a layer of abstraction for the user, but the need may arise where directly interfacing with MPI is not only beneficial, but necessary.

More details and numerous examples using MPI with \proglang{R} are available in the sections to follow, as well as in the \pkg{pbdMPI} vignette.





\section{pbdMPI vs Rmpi}

There is another package on the CRAN which the \proglang{R} programmer may use to interface with MPI, namely \pkg{Rmpi}\index{Rmpi} \citep{Rmpi}.  There are several issues one must consider when choosing which package to use if one were to only use one of them.
\begin{enumerate}
 \item $(+)$ \pkg{pbdMPI} is easier to install than \pkg{Rmpi}
 \item $(+)$ \pkg{pbdMPI} is easier to use than \pkg{Rmpi}
 \item\label{enum:perf} $(+)$ \pkg{pbdMPI} can often outperform \pkg{Rmpi}
 \item\label{enum:integrate} $(+)$ \pkg{pbdMPI} integrates with the rest of pbd
 %
 \item\label{enum:dompi} $(-)$ \pkg{Rmpi} can be used with \pkg{foreach} \citep{foreach} via \pkg{doMPI} \citep{dompi}
 \item $(-)$ \pkg{Rmpi} can be used in the master/worker paradigm
\end{enumerate}

We do not believe that the above can be reduced to a zero-sum game with unambiguous winner and loser.  Ultimately the needs of the user (or developer) are paramount.  We believe that pbd makes a very good case for itself, especially in the SPMD style, but it can not satisfy everyone.  However, for the remainder of this section, we will present the case for several of the, as yet, unsubstantiated pluses above.

In the case of ease of use, \pkg{Rmpi} uses bindings very close to the level as they are used in C's MPI API.  Specifically, whenever performing, for example, a reduction such as allreduce, you must specify the type of your data.  For example, using \pkg{Rmpi}'s API
\begin{lstlisting}[language=rr]
mpi.allreduce(x, type = 1)
\end{lstlisting}
would perform the sum allreduce if the object \code{x} consists of integer data, while
\begin{lstlisting}[language=rr]
mpi.allreduce(x, type = 2)
\end{lstlisting}
would be used if \code{x} consists of doubles.  However, with \pkg{pbdMPI}
\begin{lstlisting}[language=rr]
allreduce(x)
\end{lstlisting}
is used for both by making use of \proglang{R}'s S4 system of object oriented programming.  This is not mere code golfing\footnote{See \url{https://en.wikipedia.org/wiki/Code_golf}} that we are engaging in.  The concept of what ``type'' your data is in \proglang{R} is fairly foreign to most \proglang{R} users, and misusing the \code{type} argument in \pkg{Rmpi} is a very easy way to crash your program.  Even if you are more comfortable with statically typed languages and have no problem with this concept, consider the following example:

\begin{lstlisting}[language=rr,title=Types in R]
> is.integer(1)
[1] FALSE
> is.integer(2)
[1] FALSE
> is.integer(1:2)
[1] TRUE
\end{lstlisting}

There are good reasons for R Core to have made this choice; that is not the point.  The point is that because objects in \proglang{R} are dynamically typed, having to know the type of your data when utilizing \pkg{Rmpi} is a needless burden.  Instead, \pkg{pbdMPI} takes the approach of adding a small abstraction layer on top (which we intend to show does not negatively impact performance) so that the user need not worry about such fiddly details.

In terms of performance, \pkg{pbdMPI} can greatly outperform \pkg{Rmpi}.  We present here the results of a benchmark we performed comparing the allgather operation between the two packages \citep{pdac}.  The benchmark consisted of calling the respective allgather
function from each package on a randomly generated $10,000 \times 10,000$ distributed matrix with entries coming from the standard normal distribution, using different numbers of processors.  
\begin{table}[h]
 \centering
 \caption[Benchmark Comparing \pkg{Rmpi} and \pkg{pbdMPI}]{Benchmark Comparing \pkg{Rmpi} and \pkg{pbdMPI}.  Run time in seconds is listed for each operation.  The speedup is relative to \pkg{Rmpi}.}
 \label{tab:allgather}
 \begin{tabular}{rrrr}\hline\hline
  Cores & \pkg{Rmpi} & \pkg{pbdMPI} & Speedup \\\hline
  32    & 24.6       & 6.7          & 3.67 \\
  64    & 25.2       & 7.1          & 3.55 \\
  128   & 22.3       & 7.2          & 3.10 \\
  256   & 22.4       & 7.1          & 3.15 \\\hline\hline
 \end{tabular}
\end{table}
Table~\ref{tab:allgather} shows the results for this test, and in each case, \pkg{pbdMPI} is the clear victor.

Whichever package you choose, whichever your favorite, for the remainder of this document we will be using (either implicitly or explicitly) \pkg{pbdMPI}.  








\section{Common MPI Operations}

Fully explaining the process of MPI programming is a daunting task.  Thankfully, we can punt and merely highlight some key MPI operations and how one should use them with \pkg{pbdMPI}.



\subsection{Basic Communicator Wrangling}

First things first, we must examine basic communicator issues, like construction, destruction, and each processor's position within a communicator.

\begin{itemize}
  \item \textbf{Managing a Communicator}:  Create and destroy communicators.\\
  \code{init()} --- initialize communicator\\
  \code{finalize()} --- shut down communicator(s)
  %
  \item \textbf{Rank query}: Determine the processor's position in the communicator.\\
  \code{comm.rank()} --- ``who am I?''\\
  \code{comm.size()} --- ``how many of us are there?''
  %
  \item \textbf{Barrier}: No processor can proceed until \emph{all} processors can proceed.\\
  \code{barrier()} --- ``computation wall'' that only all processors together can tear down
\end{itemize}


One quick word before proceeding.  If a processor queries \code{comm.size()}, this will return the total number of processors in the communicators.  However, communicator indexing is like indexing in the programming language \proglang{C}.  That is, the first element is numbered 0 rather than 1.  So when the first processor queries \code{comm.rank()}, it will return 0, and when the last processor queries \code{comm.rank()}, it will return \code{comm.size() - 1}.
  
We are finally ready to write our first MPI program:

\begin{lstlisting}[language=rr,title=Simple pbdMPI Example 1]
library(pbdMPI, quiet = TRUE)
init()

myRank <- comm.rank() + 1 # comm index starts at 0, not 1
print(myRank)

finalize()
\end{lstlisting}

Unfortunately, it is not very exciting, but you have to crawl before you can drag race.  Remember that all of our programs are written in SPMD style.  So this \emph{one} single program is written, and each processor will execute the same program, but with different results, whence the name ``Single Program/Multiple Data''.  

So what does it do?  First we initialize the MPI communicator with the call to \code{init()}.  Next, we have each processor query its rank via \code{comm.rank()}.  Since indexing of MPI communicators starts at 0, we add 1 because that is what we felt like doing.  Finally we call \proglang{R}'s \code{print()} function to print the result.  This printing is not particularly clever, and each processor will be clamoring to dump its result to the output file/terminal.  We will discuss more sophisticated means of printing later.  Finally, we shut down the MPI communicator with \code{finalize()}.

If you were to save this program in the file \code{mpiex1.r} and you wished to run it with 2 processors, you would issue the command:

\begin{Command}
### (Use Rscript.exe for windows system)
mpiexec -np 2 Rscript mpiex1.r
\end{Command}

To use more processors, you modify the \code{-np} argument (``number processors'').  So if you want to use 4, you pass \code{-np 4}.

The above program technically, though not in spirit, bucks the trend of officially opening with a ``Hello World'' program.  So as not to incur the wrath of the programming gods, we offer a simple such example by slightly modifying the above program:

\begin{lstlisting}[language=rr,title=Simple pbdMPI Example 1.5]
library(pbdMPI, quiet = TRUE)
init()

myRank <- comm.rank()

if (myRank == 0){
  print("Hello, world.")
}

finalize()
\end{lstlisting}

One word of general warning we offer now is that we are taking very simple approaches here for the sake of understanding, heavily relying on function argument defaults.  However, there are all kinds of crazy, needlessly complicated things you can do with these functions.  See the \pkg{pbdMPI} reference manual for full details about how one may utilize these (and other) \pkg{pbdMPI} functions.



\subsection{Reduce, Broadcast, and Gather}

Once managing a communicator is under control, you presumably want to do things with all of your processors.  The typical way you will have the processors interact is given below:

\begin{itemize}
  \item \textbf{Reduction}:  Say each processor has a number \code{x.spmd}. Add all of them up, find the largest, find the smallest, \dots .\\
  \code{reduce(x.spmd, op='sum')} --- only one processor gets result (default is 0)\\
  \code{allreduce(x.spmd, op='sum')} --- every processor gets result
  %
  \item \textbf{Gather}: Say each processor has a number. Create a new object on some processor(s) containing all of those numbers.\\
  \code{gather(x.spmd)} --- only one processor gets result\\
  \code{allgather(x.spmd)} --- every processor gets result
  %
  \item \textbf{Broadcast}: One processor has a number \code{x.spmd} that every other processor should also have.\\
  \code{bcast(x.spmd)}
\end{itemize}

Here perhaps explanations are inferior to examples; so without wasting any more time, we proceed to the next example:

\begin{lstlisting}[language=rr,title=Simple pbdMPI Example 2]
library(pbdMPI, quiet = TRUE)
init()

n.spmd <- sample(1:10, size=1)

sm <- allreduce(n.spmd) # default op is 'sum'
print(sm)

gt <- allgather(n.spmd)
print(gt)

finalize()
\end{lstlisting}

So what does it do?  First each processor samples a number from 1 to 10; it is probably true that each processor will be using a different seed for this sampling, though you should not rely on this alone to ensure good parallel seeds.  More on this particular problem in Section~\ref{sec:pbdsugar} below.

Next, we perform an \code{allreduce()} on \code{n.spmd}.  Conceivably, the processors could have different values for \code{n.spmd}.  So the value of \code{n} is local to each processor.  So perhaps we want to add up all these numbers (with as many numbers as there are processors) and store them in the global value \code{sm} (for ``sum'').  Each processor will agree as to the value of \code{sm}, even if they disagree about the value of \code{n.spmd}.

Finally, we do the same but with an \code{allgather()} operation.  

Try experimenting with this by running the program several times.  You should get different results each time.  To make sure we have not been lying to you about what is happening, you can even print the values of \code{n.spmd} before the reduce and gather operations.



\subsection{Printing and RNG Seeds}\label{sec:pbdsugar}

In addition to the above common MPI operations, which will make up the bulk of the MPI programmer's toolbox, we offer a few extra utility functions:

\begin{itemize}
  \item \textbf{Print}: printing with control over which processor prints.\\
  \code{comm.print(x, \dots)}\\
  \code{comm.cat(x, \dots)}
  %
  \item \textbf{Random Seeds}: \\
  \code{comm.set.seed(seed, diff=FALSE)}:  every processor uses the seed \code{seed}
  \code{comm.set.seed(diff=TRUE)}: every processor uses an independent seed (via \pkg{rlecuyer})
\end{itemize}

The \code{comm.print()} and \code{comm.cat()} functions are especially handy, because they are much more sophisticated than their \code{R} counterparts when using multiple processes.  These functions which processes do the printing, and if you choose to have all processes print their result, then the printing occurs in an orderly fashion, with processor 0 getting the first line, processor 1 getting the second, and so on.

For example, revisiting our ``Hello, world'' example, we can somewhat simplify the code with a slight modification:

\begin{lstlisting}[language=rr,title=Simple pbdMPI Example 3]
library(pbdMPI, quiet = TRUE)
init()

myRank <- comm.rank()

comm.print("Hello, world.")

finalize()
\end{lstlisting}

If we want to see what each processor has to say, we can pass the optional argument \code{all,rank=TRUE} to \code{comm.print()}.  By default, each process will print its rank, then what you told it to print.  You can suppress the printing of communicator rank via the optional argument \code{quiet=TRUE} to \code{comm.print()}.

These functions are quite handy\dots
\begin{center}
\colorbox{yellow}{\Huge \color{red}{\#\#\#\#\# HOWEVER \#\#\#\#\# }}  
\end{center}
these functions are potentially dangerous, and so some degree of care should be exercised.  Indeed, it is possible to lock up all of the active \proglang{R} sessions by incorrectly using them.  Worse, achieving this behavior is fairly easy to do.  The way this occurs is by issuing a \code{comm.print()} on an expression which requires communication.  For example, suppose we have a distributed object with local piece \code{x.spmd} and a function \code{myFunction()} which requires communication between the processors.  Then calling
\begin{lstlisting}[language=rr,title=A Cautionary Tale of Printing in Parallel (1 of 3)]
print(myFunction(x.spmd)) 
\end{lstlisting}
is just fine, but will not have the nice orderly, behaved printing style of \code{comm.print()}.  However, if we issue
\begin{lstlisting}[language=rr,title=A Cautionary Tale of Printing in Parallel (2 of 3)]
comm.print(myFunction(x.spmd)) 
\end{lstlisting}
then we have just locked up all of the \proglang{R} processes.  Indeed, behind the scenes, a call somewhat akin to 
\begin{lstlisting}[language=rr]
for (rank in 0:comm.size()){
  if (comm.rank() == rank){
    # do things
  }
  barrier()
}
\end{lstlisting}
has been ordered.  The problem arises in the ``do things'' part.  Since (in our hypothetical example) the function \code{myFunction()} requires communication between the processors, it will simply wait forever for the others to respond until the job is killed.  This is because the other processors skipped over the ``do things'' part and are waiting at the barrier.  So lonely little processor 0 has been stood up, unable to communicate with the remaining processors.

To avoid this problem, make it a personal habit to only print on \emph{results}, not \emph{computations}.  We can quickly rectify the above example by doing the following:
\begin{lstlisting}[language=rr,title=A Cautionary Tale of Printing in Parallel (3 of 3)]
myResult <- myFunction(x.spmd)
comm.print(myResult)
\end{lstlisting}

In short, printing stored objects is safe.  Printing a yet-to-be-evaluated expression is not safe.





\subsection{Apply, Lapply, and Sapply}\label{sec:pbdsugar2}

But the \pkg{pbdMPI} sugar extends to more than just printing.  We also have a family of ``*ply'' functions, in the same vein as \proglang{R}'s \code{apply()}, \code{lapply()}, and \code{sapply()}:
\begin{itemize}
  \item \textbf{Apply}: *ply-like functions.\\
  \code{pbdApply(X, MARGIN, FUN, \dots)} --- analogue of \code{apply()}\\
  \code{pbdLapply(X, FUN, \dots)} --- analogue of \code{lapply()}\\
  \code{pbdSapply(X, FUN, \dots)} --- analogue of \code{sapply()}\\
\end{itemize}

Here is a simple example utilizing \code{pbdLapply()}:
    
\begin{lstlisting}[language=rr,title=Example 4]
library(pbdMPI, quiet = TRUE)
init()

n <- 100
x <- split((1:n) + n * comm.rank(), rep(1:10, each = 10))
sm <- pbdLapply(x, sum)
comm.print(unlist(sm))

finalize()
\end{lstlisting}

So what does it do?  Why don't you tell us?  We're busy people, after all, and we're not going to be around forever.  Try guessing what it will do, then run the program to see if you are correct.  As you evaluate this and every parallel code, ask yourself which pieces involve communication and which pieces are local computations.





\section{Timing MPI Tasks}

Measuring run time is a fundamental performance measure in computing.  However, in parallel computing, not all ``parallel components'' (e.g. threads, or MPI processes) will take the same amount of time to complete a task, even when all tasks are given completely identical jobs.  So measuring ``total run time'' begs the question, run time of what?

To help, we offer a timing function \code{demo.timer()} which can wrap segments of code much in the same way that \code{system.time()} does.  However, the three numbers reported by \code{demo.timer()} are: (1) the minimum elapsed time measured across all processes, (2) the average elapsed time measured across all processes, and (3) the maximum elapsed time across all processes.  The code for this function is listed below:

\begin{lstlisting}[language=rr,title=Timer Function]
demo.timer <- function(timed)
{
  ltime <- system.time(timed)[3]
  
  mintime <- allreduce(ltime, op='min')
  maxtime <- allreduce(ltime, op='max')
  
  meantime <- allreduce(ltime, op='sum') / comm.size()
  
  return( c(min=mintime, mean=meantime, max=maxtime) )
}
\end{lstlisting}



\section{Exercises}
\label{sec:mpi_for_the_r_user_exercise}

\begin{enumerate}[label=\thechapter-\arabic*]
\item
In \pkg{pbdMPI}, there is a parallel sorting function called
\code{comm.sort()} which is similar to the usual \code{sort()} of \proglang{R}.
Implement parallel equivalents to the usual \code{order()} and \code{rank()}
of \proglang{R}.

\end{enumerate}



