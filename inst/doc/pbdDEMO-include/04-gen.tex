%%% ----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{The Distributed Matrix Data Structure}

Before continuing, we must spend some time describing a new distributed data structure.  Let us return to our old friend from Section~\ref{sec:spmdstruct}:\\
\begin{align*}
x &= \left[
      \begin{array}{lllllllll}
      x_{11} & x_{12} & x_{13} & x_{14} & x_{15} & x_{16} & x_{17} & x	_{18} & x_{19}\\
      x_{21} & x_{22} & x_{23} & x_{24} & x_{25} & x_{26} & x_{27} & x	_{28} & x_{29}\\
      x_{31} & x_{32} & x_{33} & x_{34} & x_{35} & x_{36} & x_{37} & x	_{38} & x_{39}\\
      x_{41} & x_{42} & x_{43} & x_{44} & x_{45} & x_{46} & x_{47} & x	_{48} & x_{49}\\
      x_{51} & x_{52} & x_{53} & x_{54} & x_{55} & x_{56} & x_{57} & x	_{58} & x_{59}\\
      x_{61} & x_{62} & x_{63} & x_{64} & x_{65} & x_{66} & x_{67} & x	_{68} & x_{69}\\
      x_{71} & x_{72} & x_{73} & x_{74} & x_{75} & x_{76} & x_{77} & x	_{78} & x_{79}\\
      x_{81} & x_{82} & x_{83} & x_{84} & x_{85} & x_{86} & x_{87} & x	_{88} & x_{89}\\
      x_{91} & x_{92} & x_{93} & x_{94} & x_{95} & x_{96} & x_{97} & x	_{98} & x_{99}
      \end{array}
\right]_{9\times 9}
\end{align*}

but now suppose that our processors form a 2-dimensional grid of size $2\times3$:
\begin{align*}
\text{Processors = }
\left[
      \begin{array}{lll}
      \color{g11}0 & \color{g12}1 & \color{g13}2\\
      \color{g21}3 & \color{g22}4 & \color{g23}5
      \end{array}
\right] &= 
\left[
      \begin{tabular}{lll}
      \color{g11}(0,0) & \color{g12}(0,1) & \color{g13}(0,2)\\
      \color{g21}(1,0) & \color{g22}(1,1) & \color{g23}(1,2)
      \end{tabular}
\right]
\end{align*}

with the usual MPI processor rank on the left, and the corresponding
BLACS~\index{BLACS} processor grid position on the right.  But this isn't the only wrinkle.  Not only are we going to distribute our data on a 2-dimensional grid of processors (for very good reasons that are difficult to impart to the reader at this time), we are going to do so in \emph{block-cyclic} fashion.  To do this, we are going to declare a \emph{blocking factor}, which is an ordered pair, describing the decomposition size in each dimension of the matrix we wish to distribute.  The process can be difficult to imagine, so it is perhaps best to proceed with our example.

To distribute this data across our 6 processors in the form of a $2\times 3$ process grid in $2\times 2$ blocks, we go in a ``round robin'' fashion, assigning $2\times 2$ submatrices of the original matrix to the appropriate processor, starting with processor $(0, 0)$.  Then, if possible, we move on to the next $2\times 2$ block of $x$ and give it to processor $(0, 1)$.  We continue in this fashion with $(0,2)$ if necessary, and if there is yet more of $x$ in that row still without ownership, we cycle back to processor $(0,0)$ and start over, continuing in this fashion until there is nothing left to distribute in that row.

After all the data in the first two rows of $x$ has been chopped into 2-column blocks and given to the appropriate process in process-column 1, we then move onto the next 2 rows, proceeding in the same way but now using the second process row from our process grid.  For the next 2 rows, we cycle back to process row 1.  And so on and so forth.

Then distributed across processors, the data will look like:
\begin{align*}
x &= \left[
      \begin{array}{ll|ll|ll|ll|l}
      \color{g11}x_{11} & \color{g11}x_{12} & \color{g12}x_{13} & \color{g12}x_{14} & \color{g13}x_{15} & \color{g13}x_{16} & \color{g11}x_{17} & \color{g11}x_{18} & \color{g12}x_{19}\\
      \color{g11}x_{21} & \color{g11}x_{22} & \color{g12}x_{23} & \color{g12}x_{24} & \color{g13}x_{25} & \color{g13}x_{26} & \color{g11}x_{27} & \color{g11}x_{28} & \color{g12}x_{29}\\\hline
      \color{g21}x_{31} & \color{g21}x_{32} & \color{g22}x_{33} & \color{g22}x_{34} & \color{g23}x_{35} & \color{g23}x_{36} & \color{g21}x_{37} & \color{g21}x_{38} & \color{g22}x_{39}\\
      \color{g21}x_{41} & \color{g21}x_{42} & \color{g22}x_{43} & \color{g22}x_{44} & \color{g23}x_{45} & \color{g23}x_{46} & \color{g21}x_{47} & \color{g21}x_{48} & \color{g22}x_{49}\\\hline
      \color{g11}x_{51} & \color{g11}x_{52} & \color{g12}x_{53} & \color{g12}x_{54} & \color{g13}x_{55} & \color{g13}x_{56} & \color{g11}x_{57} & \color{g11}x_{58} & \color{g12}x_{59}\\
      \color{g11}x_{61} & \color{g11}x_{62} & \color{g12}x_{63} & \color{g12}x_{64} & \color{g13}x_{65} & \color{g13}x_{66} & \color{g11}x_{67} & \color{g11}x_{68} & \color{g12}x_{69}\\\hline
      \color{g21}x_{71} & \color{g21}x_{72} & \color{g22}x_{73} & \color{g22}x_{74} & \color{g23}x_{75} & \color{g23}x_{76} & \color{g21}x_{77} & \color{g21}x_{78} & \color{g22}x_{79}\\
      \color{g21}x_{81} & \color{g21}x_{82} & \color{g22}x_{83} & \color{g22}x_{84} & \color{g23}x_{85} & \color{g23}x_{86} & \color{g21}x_{87} & \color{g21}x_{88} & \color{g22}x_{89}\\\hline
      \color{g11}x_{91} & \color{g11}x_{92} & \color{g12}x_{93} & \color{g12}x_{94} & \color{g13}x_{95} & \color{g13}x_{96} & \color{g11}x_{97} & \color{g11}x_{98} & \color{g12}x_{99}\\
      \end{array}
\right]_{9\times 9}
\end{align*}
 with local storage:
\begin{align*}
\left[
      \begin{array}{ll|ll}
      \color{g11}x_{11} & \color{g11}x_{12} & \color{g11}x_{17} & \color{g11}x_{18}\\
      \color{g11}x_{21} & \color{g11}x_{22} & \color{g11}x_{27} & \color{g11}x_{28}\\\hline
      \color{g11}x_{51} & \color{g11}x_{52} & \color{g11}x_{57} & \color{g11}x_{58}\\
      \color{g11}x_{61} & \color{g11}x_{62} & \color{g11}x_{67} & \color{g11}x_{68}\\\hline
      \color{g11}x_{91} & \color{g11}x_{92} & \color{g11}x_{97} & \color{g11}x_{98}\\
      \end{array}
\right]_{5\times 4}
\left[
      \begin{array}{ll|l}
      \color{g12}x_{13} & \color{g12}x_{14} & \color{g12}x_{19}\\
      \color{g12}x_{23} & \color{g12}x_{24} & \color{g12}x_{29}\\\hline
      \color{g12}x_{53} & \color{g12}x_{54} & \color{g12}x_{59}\\
      \color{g12}x_{63} & \color{g12}x_{64} & \color{g12}x_{69}\\\hline
      \color{g12}x_{93} & \color{g12}x_{94} & \color{g12}x_{99}\\
      \end{array}
\right]_{5\times 3}
\left[
      \begin{array}{ll}
      \color{g13}x_{15} & \color{g13}x_{16}\\
      \color{g13}x_{25} & \color{g13}x_{26}\\\hline
      \color{g13}x_{55} & \color{g13}x_{56}\\
      \color{g13}x_{65} & \color{g13}x_{66}\\\hline
      \color{g13}x_{95} & \color{g13}x_{96}\\
      \end{array}
\right]_{5\times 2}
\\
\left[
      \begin{array}{ll|ll}
      \color{g21}x_{31} & \color{g21}x_{32} & \color{g21}x_{37} & \color{g21}x_{38}\\
      \color{g21}x_{41} & \color{g21}x_{42} & \color{g21}x_{47} & \color{g21}x_{48}\\\hline
      \color{g21}x_{71} & \color{g21}x_{72} & \color{g21}x_{77} & \color{g21}x_{78}\\
      \color{g21}x_{81} & \color{g21}x_{82} & \color{g21}x_{87} & \color{g21}x_{88}\\
      \end{array}
\right]_{4\times 4}
\left[
      \begin{array}{ll|l}
      \color{g22}x_{33} & \color{g22}x_{34} & \color{g22}x_{39}\\
      \color{g22}x_{43} & \color{g22}x_{44} & \color{g22}x_{49}\\\hline
      \color{g22}x_{73} & \color{g22}x_{74} & \color{g22}x_{79}\\
      \color{g22}x_{83} & \color{g22}x_{84} & \color{g22}x_{89}\\
      \end{array}
\right]_{4\times 3}
\left[
      \begin{array}{ll}
      \color{g23}x_{35} & \color{g23}x_{36} \\
      \color{g23}x_{45} & \color{g23}x_{46} \\\hline
      \color{g23}x_{75} & \color{g23}x_{76} \\
      \color{g23}x_{85} & \color{g23}x_{86} \\
      \end{array}
\right]_{4\times 2}
\end{align*}

Especially at first introduction, this can be a very confusing distribution.  However, it is a very robust, useful data structure for certain kinds of linear algebra computations.  This is the data structure of
ScaLAPACK,~\index{ScaLAPACK} which has more than proven itself in terms of performance.  

You \emph{could} use some more natural data distributions than the above.  However, this may have a substantial impact on performance, depending on the kinds of operations you wish to do.  For things that make extensive use of linear algebra --- particularly matrix factorizations --- you are probably much better off using the above kind of block-cyclic data distribution.  These different processor grid shapes are referred to as \emph{contexts}.  They are actually specialized MPI communicators.  By default, the recommended (easy) way of managing these contexts with \pkg{pbdDMAT} is to call
\begin{lstlisting}[language=rr]
library(pbdDMAT, quiet = TRUE)
init.grid()
\end{lstlisting}
The call to \code{init.grid()} will initialize three such contexts, named 0, 1, and 2.  Context 0 is a communicator with processors as close to square as possible, like above.  This can be confusing if you ever need to directly manipulate this data structure, but \pkg{pbdDMAT} contains \emph{numerous} helper methods to make this process painless, often akin to manipulating an ordinary, non-distributed \proglang{R} data structure.  Context 1 puts the processors in a 1-dimensional grid consisting of 1 row.  Continuing with our example, the processors form the grid:
\begin{align*}
\text{Processors = }
\left[
      \begin{array}{llllll}
      \color{g11}0 & \color{g12}1 & \color{g13}2 & \color{g21}3 & \color{g22}4 & \color{g23}5
      \end{array}
\right] &= 
\left[
      \begin{tabular}{llllll}
      \color{g11}(0,0) & \color{g12}(0,1) & \color{g13}(0,2) & \color{g21}(0,3) & \color{g22}(0,4) & \color{g23}(0,5)
      \end{tabular}
\right]
\end{align*}
and if we preserve the $2\times 2$ blocking factor, then the data would be distributed like so:
\begin{align*}
x &= \left[
      \begin{array}{ll|ll|ll|ll|l}
      \color{g11}x_{11} & \color{g11}x_{12} & \color{g12}x_{13} & \color{g12}x_{14} & \color{g13}x_{15} & \color{g13}x_{16} & \color{g21}x_{17} & \color{g21}x_{18} & \color{g22}x_{19}\\
      \color{g11}x_{21} & \color{g11}x_{22} & \color{g12}x_{23} & \color{g12}x_{24} & \color{g13}x_{25} & \color{g13}x_{26} & \color{g21}x_{27} & \color{g21}x_{28} & \color{g22}x_{29}\\
      \color{g11}x_{31} & \color{g11}x_{32} & \color{g12}x_{33} & \color{g12}x_{34} & \color{g13}x_{35} & \color{g13}x_{36} & \color{g21}x_{37} & \color{g21}x_{38} & \color{g22}x_{39}\\
      \color{g11}x_{41} & \color{g11}x_{42} & \color{g12}x_{43} & \color{g12}x_{44} & \color{g13}x_{45} & \color{g13}x_{46} & \color{g21}x_{47} & \color{g21}x_{48} & \color{g22}x_{49}\\
      \color{g11}x_{51} & \color{g11}x_{52} & \color{g12}x_{53} & \color{g12}x_{54} & \color{g13}x_{55} & \color{g13}x_{56} & \color{g21}x_{57} & \color{g21}x_{58} & \color{g22}x_{59}\\
      \color{g11}x_{61} & \color{g11}x_{62} & \color{g12}x_{63} & \color{g12}x_{64} & \color{g13}x_{65} & \color{g13}x_{66} & \color{g21}x_{67} & \color{g21}x_{68} & \color{g22}x_{69}\\
      \color{g11}x_{71} & \color{g11}x_{72} & \color{g12}x_{73} & \color{g12}x_{74} & \color{g13}x_{75} & \color{g13}x_{76} & \color{g21}x_{77} & \color{g21}x_{78} & \color{g22}x_{79}\\
      \color{g11}x_{81} & \color{g11}x_{82} & \color{g12}x_{83} & \color{g12}x_{84} & \color{g13}x_{85} & \color{g13}x_{86} & \color{g21}x_{87} & \color{g21}x_{88} & \color{g22}x_{89}\\
      \color{g11}x_{91} & \color{g11}x_{92} & \color{g12}x_{93} & \color{g12}x_{94} & \color{g13}x_{95} & \color{g13}x_{96} & \color{g21}x_{97} & \color{g21}x_{98} & \color{g22}x_{99}\\
      \end{array}
\right]_{9\times 9}
\end{align*}
Locally, the data is stored as follows:
\begin{align*}
\left[
      \begin{array}{ll}
      \color{g11}x_{11} & \color{g11}x_{12} \\
      \color{g11}x_{21} & \color{g11}x_{22} \\
      \color{g11}x_{31} & \color{g11}x_{32} \\
      \color{g11}x_{41} & \color{g11}x_{42} \\
      \color{g11}x_{51} & \color{g11}x_{52} \\
      \color{g11}x_{61} & \color{g11}x_{62} \\
      \color{g11}x_{71} & \color{g11}x_{72} \\
      \color{g11}x_{81} & \color{g11}x_{82} \\
      \color{g11}x_{91} & \color{g11}x_{92} 
      \end{array}
\right]_{9\times 2}
\left[
      \begin{array}{ll}
      \color{g12}x_{13}  &  \color{g12}x_{14} \\
      \color{g12}x_{23}  &  \color{g12}x_{24} \\
      \color{g12}x_{33}  &  \color{g12}x_{34} \\
      \color{g12}x_{43}  &  \color{g12}x_{44} \\
      \color{g12}x_{53}  &  \color{g12}x_{54} \\
      \color{g12}x_{63}  &  \color{g12}x_{64} \\
      \color{g12}x_{73}  &  \color{g12}x_{74} \\
      \color{g12}x_{83}  &  \color{g12}x_{84} \\
      \color{g12}x_{93}  &  \color{g12}x_{94} 
      \end{array}
\right]_{9\times 2}
\left[
      \begin{array}{ll}
      \color{g13}x_{15}  &  \color{g13}x_{16} \\
      \color{g13}x_{25}  &  \color{g13}x_{26} \\
      \color{g13}x_{35}  &  \color{g13}x_{36} \\
      \color{g13}x_{45}  &  \color{g13}x_{46} \\
      \color{g13}x_{55}  &  \color{g13}x_{56} \\
      \color{g13}x_{65}  &  \color{g13}x_{66} \\
      \color{g13}x_{75}  &  \color{g13}x_{76} \\
      \color{g13}x_{85}  &  \color{g13}x_{86} \\
      \color{g13}x_{95}  &  \color{g13}x_{96} 
      \end{array}
\right]_{9\times 2}
\left[
      \begin{array}{ll}
      \color{g21}x_{17}  &  \color{g21}x_{18} \\
      \color{g21}x_{27}  &  \color{g21}x_{28} \\
      \color{g21}x_{37}  &  \color{g21}x_{38} \\
      \color{g21}x_{47}  &  \color{g21}x_{48} \\
      \color{g21}x_{57}  &  \color{g21}x_{58} \\
      \color{g21}x_{67}  &  \color{g21}x_{68} \\
      \color{g21}x_{77}  &  \color{g21}x_{78} \\
      \color{g21}x_{87}  &  \color{g21}x_{88} \\
      \color{g21}x_{97}  &  \color{g21}x_{98} 
      \end{array}
\right]_{9\times 2}
\left[
      \begin{array}{ll}
      \color{g22}x_{19}\\
      \color{g22}x_{29}\\
      \color{g22}x_{39}\\
      \color{g22}x_{49}\\
      \color{g22}x_{59}\\
      \color{g22}x_{69}\\
      \color{g22}x_{79}\\
      \color{g22}x_{89}\\
      \color{g22}x_{99}\\
      \end{array}
\right]_{9\times 1}
\left[
      \begin{array}{ll}
      &\\
      &\\
      &\\
      &\\
      &\\
      &\\
      &\\
      &\\
      &
      \end{array}
\right]_{0\times 1}
\end{align*}

Here, the first dimension of the blocking factor is irrelevant.  All processors own either some part of \emph{all} rows, or they own nothing at all.  So the above would be the exact same data distribution if we had a blocking factor of $100\times 2$ or $2\times 2$.  However, the decomposition is still block-cyclic; here we use up everything before needing to cycle, based on our choice of blocking factor.  If we instead chose a $1\times 1$ blocking, then the data would be distributed like so:

\begin{align*}
x &= \left[
      \begin{array}{l|l|l|l|l|l|l|l|l}
      \color{g11}x_{11} & \color{g12}x_{12} & \color{g13}x_{13} & \color{g21}x_{14} & \color{g22}x_{15} & \color{g23}x_{16} & \color{g11}x_{17} & \color{g12}x_{18} & \color{g13}x_{19}\\
      \color{g11}x_{21} & \color{g12}x_{22} & \color{g13}x_{23} & \color{g21}x_{24} & \color{g22}x_{25} & \color{g23}x_{26} & \color{g11}x_{27} & \color{g12}x_{28} & \color{g13}x_{29}\\
      \color{g11}x_{31} & \color{g12}x_{32} & \color{g13}x_{33} & \color{g21}x_{34} & \color{g22}x_{35} & \color{g23}x_{36} & \color{g11}x_{37} & \color{g12}x_{38} & \color{g13}x_{39}\\
      \color{g11}x_{41} & \color{g12}x_{42} & \color{g13}x_{43} & \color{g21}x_{44} & \color{g22}x_{45} & \color{g23}x_{46} & \color{g11}x_{47} & \color{g12}x_{48} & \color{g13}x_{49}\\
      \color{g11}x_{51} & \color{g12}x_{52} & \color{g13}x_{53} & \color{g21}x_{54} & \color{g22}x_{55} & \color{g23}x_{56} & \color{g11}x_{57} & \color{g12}x_{58} & \color{g13}x_{59}\\
      \color{g11}x_{61} & \color{g12}x_{62} & \color{g13}x_{63} & \color{g21}x_{64} & \color{g22}x_{65} & \color{g23}x_{66} & \color{g11}x_{67} & \color{g12}x_{68} & \color{g13}x_{69}\\
      \color{g11}x_{71} & \color{g12}x_{72} & \color{g13}x_{73} & \color{g21}x_{74} & \color{g22}x_{75} & \color{g23}x_{76} & \color{g11}x_{77} & \color{g12}x_{78} & \color{g13}x_{79}\\
      \color{g11}x_{81} & \color{g12}x_{82} & \color{g13}x_{83} & \color{g21}x_{84} & \color{g22}x_{85} & \color{g23}x_{86} & \color{g11}x_{87} & \color{g12}x_{88} & \color{g13}x_{89}\\
      \color{g11}x_{91} & \color{g12}x_{92} & \color{g13}x_{93} & \color{g21}x_{94} & \color{g22}x_{95} & \color{g23}x_{96} & \color{g11}x_{97} & \color{g12}x_{98} & \color{g13}x_{99}\\
      \end{array}
\right]_{9\times 9}
\end{align*}

Finally, there is context 2.  This is deceivingly similar to the SPMD data structure, but the two are, in general, not comparable.  This context puts the processors in a 1-dimensional grid consisting of one column (note the transpose):
\begin{align*}
\text{Processors = }
\left[
      \begin{array}{llllll}
      \color{g11}0 & \color{g12}1 & \color{g13}2 & \color{g21}3 & \color{g22}4 & \color{g23}5
      \end{array}
\right]^T &= 
\left[
      \begin{tabular}{llllll}
      \color{g11}(0,0) & \color{g12}(1,0) & \color{g13}(2,0) & \color{g21}(3,0) & \color{g22}(4,0) & \color{g23}(5,0)
      \end{tabular}
\right]^T
\end{align*}
So here, the data would be decomposed as:
\begin{align*}
x &= \left[
      \begin{array}{lllllllll}
      \color{g11}x_{11} & \color{g11}x_{12} & \color{g11}x_{13} & \color{g11}x_{14} & \color{g11}x_{15} & \color{g11}x_{16} & \color{g11}x_{17} & \color{g11}x_{18} & \color{g11}x_{19}\\
      \color{g11}x_{21} & \color{g11}x_{22} & \color{g11}x_{23} & \color{g11}x_{24} & \color{g11}x_{25} & \color{g11}x_{26} & \color{g11}x_{27} & \color{g11}x_{28} & \color{g11}x_{29}\\\hline
      \color{g12}x_{31} & \color{g12}x_{32} & \color{g12}x_{33} & \color{g12}x_{34} & \color{g12}x_{35} & \color{g12}x_{36} & \color{g12}x_{37} & \color{g12}x_{38} & \color{g12}x_{39}\\
      \color{g12}x_{41} & \color{g12}x_{42} & \color{g12}x_{43} & \color{g12}x_{44} & \color{g12}x_{45} & \color{g12}x_{46} & \color{g12}x_{47} & \color{g12}x_{48} & \color{g12}x_{49}\\\hline
      \color{g13}x_{51} & \color{g13}x_{52} & \color{g13}x_{53} & \color{g13}x_{54} & \color{g13}x_{55} & \color{g13}x_{56} & \color{g13}x_{57} & \color{g13}x_{58} & \color{g13}x_{59}\\
      \color{g13}x_{61} & \color{g13}x_{62} & \color{g13}x_{63} & \color{g13}x_{64} & \color{g13}x_{65} & \color{g13}x_{66} & \color{g13}x_{67} & \color{g13}x_{68} & \color{g13}x_{69}\\\hline
      \color{g21}x_{71} & \color{g21}x_{72} & \color{g21}x_{73} & \color{g21}x_{74} & \color{g21}x_{75} & \color{g21}x_{76} & \color{g21}x_{77} & \color{g21}x_{78} & \color{g21}x_{79}\\
      \color{g21}x_{81} & \color{g21}x_{82} & \color{g21}x_{83} & \color{g21}x_{84} & \color{g21}x_{85} & \color{g21}x_{86} & \color{g21}x_{87} & \color{g21}x_{88} & \color{g21}x_{89}\\\hline
      \color{g22}x_{91} & \color{g22}x_{92} & \color{g22}x_{93} & \color{g22}x_{94} & \color{g22}x_{95} & \color{g22}x_{96} & \color{g22}x_{97} & \color{g22}x_{98} & \color{g22}x_{99}\\
      \end{array}
\right]_{9\times 9}
\end{align*}
 with local storage view:
\begin{align*}
\left[
      \begin{array}{lllllllll}
      \color{g11}x_{11} & \color{g11}x_{12} & \color{g11}x_{13} & \color{g11}x_{14} & \color{g11}x_{15} & \color{g11}x_{16} & \color{g11}x_{17} & \color{g11}x_{18} & \color{g11}x_{19}\\
      \color{g11}x_{21} & \color{g11}x_{22} & \color{g11}x_{23} & \color{g11}x_{24} & \color{g11}x_{25} & \color{g11}x_{26} & \color{g11}x_{27} & \color{g11}x_{28} & \color{g11}x_{29}
      \end{array}
\right]_{2\times 9}
\\
\left[
      \begin{array}{lllllllll}
      \color{g12}x_{31} & \color{g12}x_{32} & \color{g12}x_{33} & \color{g12}x_{34} & \color{g12}x_{35} & \color{g12}x_{36} & \color{g12}x_{37} & \color{g12}x_{38} & \color{g12}x_{39}\\
      \color{g12}x_{41} & \color{g12}x_{42} & \color{g12}x_{43} & \color{g12}x_{44} & \color{g12}x_{45} & \color{g12}x_{46} & \color{g12}x_{47} & \color{g12}x_{48} & \color{g12}x_{49}
      \end{array}
\right]_{2\times 9}
\\
\left[
      \begin{array}{lllllllll}
      \color{g13}x_{51} & \color{g13}x_{52} & \color{g13}x_{53} & \color{g13}x_{54} & \color{g13}x_{55} & \color{g13}x_{56} & \color{g13}x_{57} & \color{g13}x_{58} & \color{g13}x_{59}\\
      \color{g13}x_{61} & \color{g13}x_{62} & \color{g13}x_{63} & \color{g13}x_{64} & \color{g13}x_{65} & \color{g13}x_{66} & \color{g13}x_{67} & \color{g13}x_{68} & \color{g13}x_{69}
      \end{array}
\right]_{9\times 2}
\\
\left[
      \begin{array}{lllllllll}
      \color{g21}x_{71} & \color{g21}x_{72} & \color{g21}x_{73} & \color{g21}x_{74} & \color{g21}x_{75} & \color{g21}x_{76} & \color{g21}x_{77} & \color{g21}x_{78} & \color{g21}x_{79}\\
      \color{g21}x_{81} & \color{g21}x_{82} & \color{g21}x_{83} & \color{g21}x_{84} & \color{g21}x_{85} & \color{g21}x_{86} & \color{g21}x_{87} & \color{g21}x_{88} & \color{g21}x_{89}
      \end{array}
\right]_{9\times 2}
\\
\left[
      \begin{array}{lllllllll}
      \color{g22}x_{91} & \color{g22}x_{92} & \color{g22}x_{93} & \color{g22}x_{94} & \color{g22}x_{95} & \color{g22}x_{96} & \color{g22}x_{97} & \color{g22}x_{98} & \color{g22}x_{99}\\
      \end{array}
\right]_{9\times 1}
\\
\left[
      \begin{array}{l}
      \hspace{7.65cm}
      \end{array}
\right]_{1\times 0}
\end{align*}


So to summarize this data structure:
\begin{enumerate}
  \item \code{DMAT} is \emph{distributed}.  No one processor owns all of the matrix. \label{enum:dmat1}
  \item \code{DMAT} is \emph{non-overlapping}. Any piece owned by one processor is owned by no other processors.\label{enum:dmat2} \item \code{DMAT} can be row-contiguous or not, depending on the blocking factor used.
  \item \code{DMAT} is locally column-major and globally, it depends\dots
  \item \code{DMAT} is confusing, but very robust and useful for matrix algebra (and thus most non-trivial statistics).
\end{enumerate}

The only items in common between SPMD and DMAT are items \ref{enum:dmat1} and \ref{enum:dmat2}.  A full characterization can be given as follows.  Let $X$ be a distributed matrix with $n$ (global) rows and $p$ (global) columns.  Suppose we distribute this matrix onto a set of $nprocs$ processors in context 2 using a blocking factor $b=(b_1, b_2)$.  Then SPMD is a generalization of DMAT \emph{if and only if} we have $b_1 > \frac{n}{nprocs}$.  Otherwise, there is no relationship between these two structures (and converting between them is difficult).

In the sections to follow, we offer numerous examples utilizing this data structure.  The dedicated reader can find more information about these contexts and utilizing the DMAT data structure, see the \pkg{pbdBASE}~\citep{Schmidt2012pbdBASEvignette} and \pkg{pbdDMAT}~\citep{Schmidt2012pbdDMATvignette} vignettes.  Additionally, you can experiment more with different kinds of block-cyclic data distributions on 2-dimensional processor grids using
a very useful website at
\url{http://acts.nersc.gov/scalapack/hands-on/datadist.html}.




\section{Exercises}
\label{sec:statistics_exercise}

\begin{enumerate}[label=\thechapter-\arabic*]
\item
Read two papers given at
\url{http://acts.nersc.gov/scalapack/hands-on/datadist.html}.
``The Design of Linear Algebra Libraries for High Performance Computers'',
by J. Dongarra and D. Walker, and
``Parallel Numerical Linear Algebra'',
by J. Demmel, M. Heath, and H. van der Vorst.

\end{enumerate}




%%% ----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Constructing Distributed Matrices}

The \pkg{pbdBASE} and \pkg{pbdDMAT} packages offer a distributed matrix class, \code{ddmatrix}, as well as a collection of high-level methods for performing common matrix operations.  For example, if you want to compute the mean of an \proglang{R} matrix \code{x}, you would call 
\begin{lstlisting}[language=rr]
mean(x)
\end{lstlisting}
That's exactly the same command you would issue if \code{x} is no longer an ordinary \proglang{R} matrix, but a distributed matrix.  These methods range from simple, embarrassingly parallel operations like sums and means, to tightly coupled linear algebra operations like matrix-matrix multiply and singular value decomposition.

Unfortunately, these higher methods come with a different cost:  getting the data into the right format, namely the distributed matrix data structure DMAT, discussed at length in the previous chapter.  That said, once the hurdle of getting the data into the ``right format'' is out of the way, these methods offer very simple syntax (designed to mimic \proglang{R} as closely as possible), with the ability to scale computations on very large distributed machines.  But to get to the fun stuff, the process of exactly how to decompose data into a block-cyclic distribution must be addressed.  We begin dealing with this issue in the simplest way possible.  




\section{Fixed Global Dimension}

In these examples, we will examine the case where you know ahead of time what the global number of rows and columns are.

\subsection{Constructing Simple Distributed Matrices}

It is possible to construct fairly simple distributed matrices much in the same way that one can construct simple matrices in \proglang{R}.  We can do this using the functions
\code{ddmatrix()}\index{code!\code{ddmatrix()}} and
\code{as.ddmatrix()}.\index{code!\code{as.ddmatrix()}}
The former essentially behaves identically to \proglang{R}'s own \code{matrix()} function.  This function takes a global input vector/matrix \code{data=}, as well as the global number of rows \code{nrow=} and the global number of columns \code{ncol=}.  Additionally, the user may specify the blocking factor \code{bldim=} and the BLACS~\index{BLACS} context \code{CTXT}, and the return is a distributed matrix.  For instance, we can specify
\begin{lstlisting}[language=rr,title=ddmatrix()]
dx <- ddmatrix(data=0, nrow=10, ncol=10)
\end{lstlisting}
to get a distributed matrix with \emph{global} dimension $10\times 10$ consisting of zeros.  We can also do cute things like
\begin{lstlisting}[language=rr,title=ddmatrix()]
dx <- ddmatrix(data=1:3, nrow=5, ncol=5)
\end{lstlisting}
which will create the distributed analogue of
\begin{verbatim}
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    3    2    1    3
[2,]    2    1    3    2    1
[3,]    3    2    1    3    2
[4,]    1    3    2    1    3
[5,]    2    1    3    2    1
\end{verbatim}
How exactly that ``distributed analogue'' will look (locally) depends on the processor grid shape (whence too, the number of processors) as well as the blocking factor.  This operation performs no communication.

While this can be useful, it is far from the only way to construct distributed matrices.  One can also convert a global (non-distributed) matrix into a distributed matrix.  There are some caveats; this matrix must either be owned in total by all processors (which is very useful in testing, but should not be used at scale), or the matrix is owned in total by one processor, with all others owning \code{NULL} for that object.  

For example, we can create identical return to the above via
\begin{lstlisting}[language=rr,title=as.ddmatrix()]
x <- matrix(data=1:3, nrow=5, ncol=5)
dx <- as.ddmatrix(x)
\end{lstlisting}
or
\begin{lstlisting}[language=rr,title=as.ddmatrix()]
if (comm.rank()==0){
  x <- matrix(data=1:3, nrow=5, ncol=5)
} else {
  x <- NULL
}

dx <- as.ddmatrix(x)
\end{lstlisting}
Each of these operations performs communication.

Other, more general combinations are possible through other means, but they are much more cumbersome.




\subsection{Diagonal Distributed Matrices}\label{sub:diag}

\emph{Example:  construct \textbf{diagonal} distributed matrices of specified
global dimension.}

The demo command is
\begin{Command}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(randmat_diag_global,'pbdDEMO',ask=F,echo=F)"
\end{Command}

In \proglang{R}, the \code{diag()} function serves two purposes; namely, it is both a reduction operation and a reverse-reduction operation, depending on the input.  More specifically, if given a matrix, it produces a vector containing the diagonal entries of that matrix; but if given a vector, it constructs a diagonal matrix whose diagonal is that vector.  And so for example, the zero and identity matrices of any dimension can quickly be constructed via:
\begin{lstlisting}[language=rr,title=Diagonal Matrices in R]
diag(x=0, nrow=10, ncol=10) # zero matrix
diag(x=1, nrow=10, ncol=10) # identity matrix
\end{lstlisting}

Both of the above functionalities of \code{diag()} are available for distributed matrices; however we will only focus on the latter. 

When you wish to construct a diagonal distributed matrix, you can easily do so by using the additional \code{type=} argument to our \code{diag()} method.  By default, \code{type="matrix"}, though the user may specify \code{type="ddmatrix"}.  If so, then as one might expect, the optional \code{bldim=} and \code{ICTXT=} arguments are available.  So with just a little bit of tweaking, the above example becomes:
\begin{lstlisting}[language=rr,title=Diagonal Matrices in pbdR]
diag(x=0, nrow=10, ncol=10, type="ddmatrix") # zero (distributed) matrix
diag(x=1, nrow=10, ncol=10, type="ddmatrix") # identity (distributed) matrix
\end{lstlisting}
In fact, the \code{type=} argument employs partial matching, so if we really want to be lazy, then we could simply do the following:
\begin{lstlisting}[language=rr,title=Diagonal Matrices in pbdR]
diag(x=0, nrow=10, ncol=10, type="d") # zero (distributed) matrix
diag(x=1, nrow=10, ncol=10, type="d") # identity (distributed) matrix
\end{lstlisting}

Beyond the above brief explanation, the demo for this functionality is mostly self-contained, although we do employ the \code{redistribute()} function to fully show off local data storage.  This function is explained in detail in Chapter~\ref{sec:redist}.






\subsection{Random Matrices}\label{subsec:rng.gl}

\emph{Example:  randomly generate distributed matrices with random normal data of specificed global dimension.}

The demo command is
\begin{Command}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(randmat_global,'pbdDEMO',ask=F,echo=F)"
\end{Command}

This demo shows 3 separate ways that one can generate a random normal matrix with specified global dimension.  The first two generate the matrix in full on at least one processor and distribute(s) the data, while the last method generates locally only what is needed.  As such, the first two can be considered demonstrations with what to do when you have data read in on one processor and need to distribute it out to the remaining processors, but for the purposes of building a randomly generated distributed matrix, they are not particularly efficient strategies.

As described in the previous section, if we have a matrix \code{x} stored on processor 0 and \code{NULL} on the others, then we can distribute it out as an object of class \code{ddmatrix} via the command \code{as.ddmatrix()}.  For example
\begin{lstlisting}[language=rr]
if (comm.rank()==0){
  x <- matrix(rnorm(100), nrow=10, ncol=10)
} else {
  x <- NULL
}

dx <- as.ddmatrix(x)
\end{lstlisting}

will distribute the required data to the remaining processors.  We note for clarity that this is not equivalent to sending the full matrix to all processors and then throwing away all but what is needed.  Only the required data is communicated to the processors.

That said, having all of the data on all processors can be convenient while testing, if only for being more minimalistic in the amount of code/thinking required.  To do this, one need only do the following:

\begin{lstlisting}[language=rr]
x <- matrix(rnorm(100), nrow=10, ncol=10)

dx <- as.ddmatrix(x)
\end{lstlisting}

Here, each processor generates the full, global matrix, then throws away what is not needed.  Again, this is not efficient, but the code is concise, so it is extremely useful in testing.  Now, this assumes you are using the same seed on each processor.  This can be managed using the \pkg{pbdMPI} function \code{comm.set.seed()}, as in the demo script.  For more information, see that package's documentation.

Finally, you can generate locally only what you need.  The demo script does this via the \pkg{pbdDMAT} package's \code{ddmatrix()} function.  This is ``new'' behavior for this syntax (if you view \code{ddmatrix()} as an extension of \code{matrix()}).  Ordinarily you would merely execute something like
\begin{lstlisting}[language=rr,title=Creating a random normal matrix in serial R]
x <- rnorm(n*p)
x <- matrix(x, nrow=n, ncol=p) # this creates a copy

y <- rnorm(n*p)
dim(y) <- c(n, p) # this does not
\end{lstlisting}
However, things are slightly more complicated with \code{ddmatrix} objects, and the user may not easily know ahead of time what the size of the local piece is just from knowing the global dimension.  Because this requires a much stronger working knowledge of the underlying data structure than most will be comfortable with, we provide this simple functionality as an extension.  However, we note that the disciplined reader is more than capable of figuring out how it functions by examining the source code and checking with the reference manual.
% 
% The concise explanation is that the \code{base.numroc()} utility determines the size of the local storage.  This is all very well documented in the \pkg{pbdBASE} documentation, but since no one even pretends to read that stuff, \texttt{NUMROC} is a ScaLAPACK tool, which means ``\texttt{NUM}ber of \texttt{R}ows \texttt{O}r \texttt{C}olumns.''  The function \code{base.numroc()} is an implementation in \proglang{R} which calculates the number of rows \emph{and} columns at the same time (so it is a bit of a misnomer, but preserved for historical reasons).  
% 
% More precisely, it calculates the local storage requirements given a global dimension \code{dim}, a blocking factor \code{bldim}, and a BLACS context number \code{ICTXT}.  The extra argument \code{fixme} determines whether or not the lowest value returned should be 1.  If \code{fixme==FALSE} and any of the returned local dimensions are less than 1, then that processor does not actually own any of the global matrix --- it has no local storage.  But something must be stored, and so we default this to \code{matrix(0)}, the $1\times 1$ matrix with single entry 0.





\section{Fixed Local Dimension}

\emph{Example:  randomly generate distributed matrices with random normal data of specificed local dimension.}

The demo command is
\begin{Command}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(randmat_local,'pbdDEMO',ask=F,echo=F)"
\end{Command}

This is similar to the above, but with a critical difference.  Instead of specifying a fixed \emph{global} dimension and then go determine what the local storage space is, instead we specify a fixed \emph{local} dimension and then go figure out what the global dimension should be.  This can be useful for testing weak scaling of an algorithm, where different numbers of cores are used with the same local problem size.

To this end, the demo script utilizes the \code{ddmatrix.local()} function, which has the user specify a local dimension size that all the processors should use, as well as a blocking factor and BLACS~\index{BLACS} context.  Now here things get somewhat tricky, because in order for this matrix to exist at all, each margin of the blocking factor must divide (as an integer) the corresponding margin of the global dimension.  To better understand why this is so, the reader is suggested to read the \pkg{pbdDMAT} vignette.  But if you already understand or are merely willing to take it on faith, then you surely grant that this is a problem.

So here, we assume that the local dimension is chosen appropriately by the user, but it is possible that a bad blocking factor is supplied by the user.  Rather than halt with a stop error, we attempt to find the next best blocking factor possible.  To do this, we must find the smallest integer above the specified blocking factor that will divide the number of local rows or columns.




\section{Exercises}
\label{sec:statistics_exercise}

\begin{enumerate}[label=\thechapter-\arabic*]
\item
Random number generation (RNG) is used in this Section such as \code{rnorm()}.
In \proglang{pbdR}, we use an \proglang{R} package
\pkg{rlecuyer}~\citep{rlecuyer}\index{package!\pkg{rlecuyer}}
to set different streams of seed in
parallel. Try to find and use other RNG methods or implementations
in \proglang{R}.

\end{enumerate}
