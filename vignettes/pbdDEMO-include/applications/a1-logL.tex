
\chapter{Likelihood}
\label{chp:likelihood}


\inspire{It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts.}{Sherlock Holmes}


\section{Introduction}
\label{sec:likelihood_introduction}

This is a preamble chapter for Chapters~\ref{chp:pmclust}
and~\ref{chp:phyloclustering} where applications are heavily relied on
likelihood functions\index{likelihood function} which is one of the most
important modern Statistical technique.
The ``likelihood'' was popularized
in mathematical statistics by R.A. Fisher in 1922:
``On the mathematical foundations of theoretical
statistics''~\citep{Fisher1922}.
In short, the likelihood is a way based on data to build up
theoretical inference for the facts.

We introduce general notations for likelihood function
which is a standard method for parametric statistics and is
useful for statistical inference~\citep{Casella2001}.
Two useful distributions are introduced.
The normal distribution additional to linear model has been applied to
the example in Section~\ref{sec:ols}.
The multivariate normal
distribution\index{Distribution!multivariate normal distribution}
is also popular to model high dimensional data such as
model-based clustering in Chapter~\ref{chp:pmclust}.

Suppose $\bX = \{\bX_1, \bX_2, \ldots, \bX_N\}$ is a random sample,
which means independent identically distributed
(i.i.d.),\index{i.i.d.}
from a population characterized by a distribution $\mF(\btheta)$ with unknown
parameter $\btheta \in \bTheta$ where $\bTheta$ is the parameter space.
Suppose further $\mF$ has a probability density function
(pdf)\index{pdf}
$f(\bX_n; \btheta)$ provided an appropriate support.
The goal is to estimate $\btheta$ based on the observed data
$\bx = \{\bx_1, \bx_2, \ldots, \bx_N\}$.
Ideally, we want to infer what is the best candidate of $\btheta$
where $\bx$ is observed from.
Unlike in Mathematics, $\bx$ is known, but $\btheta$ is unknown
to be determined in Statistics.

Typically, a fancy way to estimate $\btheta$ is based on the
likelihood function for the observed data $\bx$
\begin{equation}
L(\btheta; \bx) = \prod_{n = 1}^N f(\bx_n; \btheta)
\label{eqn:likelihood}
\end{equation}
or the log likelihood function
\begin{equation}
\log L(\btheta; \bx) = \sum_{n = 1}^N \log f(\bx_n; \btheta).
\label{eqn:log_likelihood}
\end{equation}
The product of Equation~(\ref{eqn:likelihood}) is due to the independent
assumption of $\bX$, but the $L(\btheta; \bx)$ may blow to infinity
or negative infinity quickly as sample size $N$ increased. While,
Equation~(\ref{eqn:log_likelihood}) has some
better properties for most distribution families and
is more numerically stable than Equation~(\ref{eqn:likelihood}).
We then either analytically or numerically maximize
Equation~(\ref{eqn:log_likelihood})
over $\bTheta$ to obtain a maximum likelihood estimation
(MLE)\index{MLE}
$$
  \hat{\btheta}_{ML} := \argmax_{\btheta\in\bTheta} \log L(\btheta; \bx)
$$
which exists in fairly-wide assumptions
such as regularity conditions of parameter space
and parameter does not depend on support,
see \citet{Casella2001} for details.




\section{Normal Distribution}
\label{sec:normal_distribution}

Section~\ref{sec:ols} is one way to find
$\btheta = \{\bbeta,\sigma^2\}$ for a linear model
without parametric assumption via ordinary least square estimator
$\hat{\btheta}_{ols} = \{\hat{\bbeta}_{ols},\hat{\sigma}_{ols}^2\}$.
Beside Gauss-Markov Theorem\index{Gauss-Markov Theorem},
an alternative way is based on likelihood approach by
assuming an identical normal distribution with mean zero and
variance $\sigma^2$ to the independent error terms of
Equation~(\ref{math:statslls}).
This implies a normal distribution\index{Distribution!normal distribution}
to the response $y_n$ for $n=1,2,\ldots, N$ that
\begin{equation}
y_n \stackrel{i.i.d}{\sim} N(\bx_n^\top\bbeta, \sigma^2)
\label{eqn:normal}
\end{equation}
where $\btheta = \{\bbeta, \sigma^2\}$, and
$\bbeta$ and $\bx_n$ has dimension $p\times 1$.

One may construct a log likelihood based on the normal density function as
\begin{equation}
\log L(\bbeta, \sigma^2; \by) = \sum_{n = 1}^N
\left[
-\frac{1}{2} \log (2\pi \sigma^2) -
\frac{(y_n - \bx_n^\top\bbeta)^2}{2\sigma^2}
\right].
\label{eqn:log_likelihood_normal}
\end{equation}
The MLEs
$\hat{\btheta}_{ML} = \{\hat{\beta}_{ML}, \hat{\sigma}^2_{ML}\}$
can be obtained analytically for this case by
taking the first derivatives of Equation~(\ref{eqn:log_likelihood_normal}),
setting them to zero, and
solving the equations.
The implementations for numerical solutions (from analytical solutions)
or numerical optimization
of Equation~(\ref{eqn:log_likelihood_normal}) is not difficult
and leaved in Exercise~\ref{ex:likelihood1}.

The assumptions of Statement~(\ref{eqn:normal}) limit the modeling capability.
We introduce a more general approach next for better
modeling approach.
Since the independent assumption and Multivariate Statistics, the
Statement~(\ref{eqn:normal})
implies a multivariate normal distribution
(MVN)\index{Distribution!multivariate normal distribution}\index{Distribution!MVN}
(introduced in Section~\ref{sec:mvn})
to response variables $\by$ with dimension $N\times 1$ that
\begin{equation}
\by \sim MVN_N(\bmu, \bSigma).
\label{eqn:mvn_n}
\end{equation}
where $\bmu = \bX\bbeta$ with length $N$,
$\bSigma = \sigma^2 \bI$ and $\bI$ is an $N\times N$ identity matrix.
In this case, the $\by$ has a density function as
\begin{equation*}
\displaystyle
\phi_N(\by; \bmu, \bSigma) =
(2\pi)^{-\frac{N}{2}} |\bSigma|^{-\frac{1}{2}}
e^{-\frac{1}{2} (\by - \bmu)^\top \bSigma^{-1} (\by - \bmu)}
\end{equation*}
and the log likelihood can reduce to
Equation~(\ref{eqn:log_likelihood_normal}).
The MLEs are
$\hat{\bbeta}_{ML} = (\bX^\top \bX)^{-1} \bX^\top \by$ and
$\sigma^2_{ML} = \frac{1}{N} (\by - \bar{y}\bone)^\top(\by - \bar{y}\bone)$
where $\bar{y}$ is the average of $\by$,
and $\bone$ is an one vector with length $N$.




\section{Likelihood Ratio Test}
\label{sec:lrt}

An advance statistical inference tool is based on likelihood ratio test
(LRT)\index{likelihood ratio test}\index{LRT} provided suitable assumption
holds. Suppose we have data $\bX$ and want to test a hypothesis
$$
  H_0: \btheta \in \bTheta_0 \mbox{ v.s. }
  H_a: \btheta \in \bTheta_a
$$
where $\bTheta_0 \neq \bTheta_a$ that two spaces are not equivalent.
The LRT says
\begin{equation}
-2\log \Lambda(\btheta_0, \btheta_a; \bX) :=
-2\log
\frac{
\max_{\btheta\in\bTheta_0} L(\btheta; \bX)
}{
\max_{\btheta\in\bTheta_0\cup\bTheta_a} L(\btheta; \bX)
}
\sim \chi^2_p
\label{eqn:LRT}
\end{equation}
where $\btheta_0$ and $\btheta_a$ are parameters that has the maximum
likelihoods on spaces $\bTheta_0$ and $\bTheta_0\cup\bTheta_a$, and
$\chi^2_p$ is a chi-squared distribution\index{Distribution!chi-squared}
with $p$ degrees of freedom. In some case, the $p$ can be simplified as
the number of dimension difference of $\bTheta_0\cup\bTheta_a$ and
$\bTheta_0$.

For example,
in the least square case, such as Statement~(\ref{eqn:mvn_n}),
we may want to know
$$
  H_0: \bsigma^2 = 1 \mbox{ v.s. }
  H_a: \bsigma^2 > 0 
$$
which means $\bTheta_0 = \{\bbeta\}$ and $\bTheta_a = \{\bbeta, \sigma^2\}$.
Note that $\bTheta_0 \subset \bTheta_a$, so
$\bTheta_0 \cup \bTheta_a = \bTheta_a$.
Given the MLEs $\hat{\btheta}_{0\,ML}$ and $\hat{\btheta}_{a\,ML}$
on two spaces $\bTheta_0$ and $\bTheta_a$, respectively.
The LRT will be
$$
  -2\log \hat{\Lambda}(\hat{\btheta}_{0\,ML}, \hat{\btheta}_{a\,ML}; \bX) :=
  -2\log
  \frac{
    L(\hat{\btheta}_{0\,ML}; \bX)
  }{
    L(\hat{\btheta}_{a\,ML}; \bX)
  }
  \sim \chi^2_1.
$$
For type I error $\alpha = 0.05$, if
the value
$$
  -2\log \hat{\Lambda}(\hat{\btheta}_{0\,ML}, \hat{\btheta}_{a\,ML}; \bX)
  > q_{\chi^2_1}(0.95) \approx 3.84
$$
where $q_{\chi^2_1}(0.95)$ is the 95\% quantile of chi-squared distribution
with 1 degree of freedom, then
we may reject $H_0: \bsigma^2 = 1$ provided type I error is no greater
than $0.05$ level.

Note that the LRT introduced here is not dependent on the types of
distributions, but has nested parameter space restriction and some
regular conditions of parameter space.
See \citet{Casella2001} or \citet{Ferguson1996} for more details of LRTs.




\section{Multivariate Normal Distribution}
\label{sec:mvn}

Suppose $\{\bX_1,\bX_2,\ldots,\bX_N\}$ is a random sample from
multivariate normal distribution
(MVN)\index{Distribution!multivariate normal distribution}\index{Distribution!MVN}
\begin{equation}
\bX_n \stackrel{i.i.d}{\sim} MVN_p(\bmu, \bSigma)
\label{eqn:mvn_p}
\end{equation}
where $\btheta = \{\bmu, \bSigma\}$, $\bmu$ is a center with
dimension $p\times 1$, and $\bSigma$ is a $p\times p$ dispersion
matrix.
The $\bX_n$ has a density function as
\begin{equation*}
\displaystyle
\phi_p(\bx_n; \bmu, \bSigma) =
(2\pi)^{-\frac{p}{2}} |\bSigma|^{-\frac{1}{2}}
e^{-\frac{1}{2} (\bx_n - \bmu)^\top \bSigma^{-1} (\bx_n - \bmu)}.
\end{equation*}
In general, $\bSigma$ could be an unstructured dispersion and must be positive
definite. Excepting over fitting problems,
an unstructured dispersion $\bSigma$ is desirable to fully
characterize correlation of dimensions since the estimation of
$\bSigma$ is completely supported by observed data and there is no
restriction on any coordinate of parameter space.

Let $\bx = (\bx_1^\top, \bx_2^\top, \ldots, \bx_N^\top)^\top$
be an observed data matrix with dimension $N\times p$.
The log likelihood function for $N$ observations is
\begin{equation*}
\displaystyle
\log L(\bmu, \bSigma; \bx) = \sum_{n = 1}^N
-\frac{1}{2}
\left[
p\log(2\pi) + \log|\bSigma| + (\bx_n - \bmu)^\top \bSigma^{-1} (\bx_n - \bmu)
\right].
\label{eqn:log_likelihood_mvn_p}
\end{equation*}
The problem is the computing time grows as $N$ and $p$ increased.
In some numerically cases, such as model-based clustering
in Chapter~\ref{chp:pmclust}, the total log likelihood is
repeated computed in each iteration for all samples and all components.

Suppose $\bmu$ and $\bSigma$ are known, and no over- and under-flow,
an efficient way is given in next
\begin{lstlisting}[language=rr,title=R Code]
U <- chol(SIGMA)
logdet <- sum(log(abs(diag(U)))) * 2
B <- sweep(X.gbd, 2, MU) %*% backsolve(U, diag(1, p))

# The over- and under-flow need extral care after this step.
distval.gbd <- rowSums(B * B)

distval <- allreduce(sum(distval.gbd))
total.logL <- -(p * log(2 *pi) + logdet + distval) * 0.5
\end{lstlisting}
where \code{X.gbd} is a GBD row-major matrix with dimension
\code{N.gbd} by \code{p}, \code{MU} is a vector with length \code{p}, and
\code{SIGMA} is a \code{p} by \code{p} positive definite matrix.
The sample size $N$ will be the sum of \code{N.gbd} across all processors.
Note that this trick of computing log likelihood is an one-pass implementation
of \code{X.gbd}, \code{MU}, and \code{SIGMA}.
See HPSC~\citep{hpsc2011} or \citet{gvl} for more details.




\section{Exercises}
\label{sec:mle_exercise}

\begin{enumerate}[label=\thechapter-\arabic*]

\item
What is the definition of ``independent identical distributed''?

\item
What is the definition of ``probability density function''?

\item
Suppose $g(\cdot)$ is a continuous function provided appropriate support,
argue that $g\left(\hat{\theta}_{ML}\right)$ is still a maximum likelihood
estimator of $g(\theta)$.

\item
Derive MLEs from Equation~(\ref{eqn:log_likelihood_normal}).

\item
As Exercise~\ref{ex:stat1}, argue that $\hat{\bbeta}_{ML}$ of
Equation~(\ref{eqn:log_likelihood_normal}) is also
an unbiased estimator of $\bbeta$.

\item
Argue that $\hat{\sigma}^2_{ML}$ of
Equation~(\ref{eqn:log_likelihood_normal})
is a biased estimator, but it is an asymptotic
unbiased estimator of $\sigma^2$.

\item
Assume data are stored in GBD row-major matrix format,
implement an optimization function for
Equation~(\ref{eqn:log_likelihood_normal}), numerically optimized via
\code{optim()}\index{Code!\code{optim()}} in \proglang{R}.
Verify the results with the analytical solution.
\label{ex:likelihood1}

\item
Argue that Statement~(\ref{eqn:normal}) implies Statement~(\ref{eqn:mvn_n})
provided appropriated assumption hold.

\item
Give an example that $X$ and $Y$ are both have a normal distribution
but $(X, Y)$ is not a multivariate normal distribution.

\item
Give an example that $(X, Y)$ has a multivariate normal distribution,
but $X$ and $Y$ do not have an independent normal distribution.

\item
Prove Statement~(\ref{eqn:LRT}).
{\color{blue} Hint: \citet{Ferguson1996}.}

\item
Implement similar trick of Section~\ref{sec:mvn} to Principal Component
Analysis (PCA).
\index{Principal Components Analysis|see{PCA}}\index{PCA}
{\color{blue} Hint: HPSC~\citep{hpsc2011}.}


\end{enumerate}

