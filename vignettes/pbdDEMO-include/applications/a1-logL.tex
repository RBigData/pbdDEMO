
\chapter{Likelihood}
\label{chp:likelihood}


\inspire{It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts.}{Sherlock Holmes}


\section{Introduction}
\label{sec:likelihood_introduction}

This is a preamble chapter for Chapters~\ref{chp:pmclust}
and~\ref{chp:phyloclustering}, each of which heavily rely on
likelihood functions\index{likelihood function}.  In a very real sense, 
likelihoods form the dividing line that separates statistics from other 
fields, and as such is one of the most important Statistical 
techniques.  The concept of likelihood was popularized
in mathematical statistics by R.A. Fisher in 1922 in his landmark paper
``On the mathematical foundations of theoretical
statistics''~\citep{Fisher1922}.
In condensed, broad strokes, likelihood is a tool for developing theoretical 
inference based on observed data.

We introduce general notations for likelihood functions,
which is a standard method for parametric statistics, and is
useful for statistical inference~\citep{Casella2001}.
Two useful distributions are introduced.
The normal distribution additional to linear model has been applied to
the example in Section~\ref{sec:ols}.
The multivariate normal
distribution\index{Distribution!multivariate normal distribution}
is also popular to model high dimensional data, and is often used in methods 
such as model-based clustering in Chapter~\ref{chp:pmclust}.

Suppose $\bX = \{\bX_1, \bX_2, \ldots, \bX_N\}$ is a random sample,
which means the observations are independent and identically distributed
(i.i.d.),\index{i.i.d.}
from a population characterized by a distribution $\mF(\btheta)$ with unknown
parameter $\btheta \in \bTheta$, where $\bTheta$ is the parameter space.
Suppose further $\mF$ has a probability density function
(pdf for short)\index{pdf}
$f(\bX_n; \btheta)$, with appropriate support.
The goal is to estimate $\btheta$ based on the observed data
$\bx = \{\bx_1, \bx_2, \ldots, \bx_N\}$.
Ideally, we want to infer what is the best candidate of $\btheta$
from which we observed $\bx$.
Unlike in Mathematics, $\bx$ is known, but $\btheta$ is unknown and
to be determined in Statistics.

A fancy way to estimate $\btheta$ is based on the
likelihood function for the observed data $\bx$
\begin{equation}
L(\btheta; \bx) = \prod_{n = 1}^N f(\bx_n; \btheta)
\label{eqn:likelihood}
\end{equation}
or the log likelihood function
\begin{equation}
\log L(\btheta; \bx) = \sum_{n = 1}^N \log f(\bx_n; \btheta).
\label{eqn:log_likelihood}
\end{equation}
The product on the right hand side of Equation~(\ref{eqn:likelihood}) is due to 
the independence assumption of $\bX$, but the value of $L(\btheta; \bx)$ may 
``blow up'' to infinity
or negative infinity quickly as sample size $N$ increased. Note that typically, 
one does not work with the likelihood function in the form of 
Equation~(\ref{eqn:likelihood}), but rather the log likelihood of
Equation~(\ref{eqn:log_likelihood}).  The reason for this is that the latter 
has some nicer properties for most distribution families and
is more numerically stable than Equation~(\ref{eqn:likelihood}).

Statistical methods that deal (directly) with likelihoods involve 
maximizing (analytically or numerically if the former is not possible or 
impractical) Equation~(\ref{eqn:log_likelihood})
over the parameter space $\bTheta$ to obtain a so-called maximum likelihood 
estimation, or MLE\index{MLE}
$$
  \hat{\btheta}_{ML} := \argmax_{\btheta\in\bTheta} \log L(\btheta; \bx)
$$
Note that the MLE may not exist.  There 
are some additional constraints that are often imposed which make a MLE more 
well-behaved in some regards, such as regularity conditions of parameter space, 
or that the parameter $\btheta$ does not depend on the pdf's support.  See 
\citet{Casella2001} for details.




\section{Normal Distribution}
\label{sec:normal_distribution}

Section~\ref{sec:ols} offers one way to find
$\btheta = \{\bbeta,\sigma^2\}$ for a linear model
without parametric assumption via ordinary least square estimator
$\hat{\btheta}_{ols} = \{\hat{\bbeta}_{ols},\hat{\sigma}_{ols}^2\}$.
Aside from the Gauss-Markov Theorem\index{Gauss-Markov Theorem},
an alternative way is based on likelihood approach by
assuming an identical normal distribution with mean zero and
variance $\sigma^2$ to the independent error terms of
Equation~(\ref{math:statslls}).
This implies a normal distribution\index{Distribution!normal distribution}
to the response $y_n$ for $n=1,2,\ldots, N$.  More precisely,
\begin{equation}
y_n \stackrel{i.i.d.}{\sim} N(\bx_n^\top\bbeta, \sigma^2)
\label{eqn:normal}
\end{equation}
where $\btheta = \{\bbeta, \sigma^2\}$, and
$\bbeta$ and $\bx_n$ each have dimension $p\times 1$.

By merely mechanically inserting symbols, one may construct a log likelihood 
function based on the normal density function:
\begin{equation}
\log L(\bbeta, \sigma^2; \by) = \sum_{n = 1}^N
\left[
-\frac{1}{2} \log (2\pi \sigma^2) -
\frac{(y_n - \bx_n^\top\bbeta)^2}{2\sigma^2}
\right].
\label{eqn:log_likelihood_normal}
\end{equation}
The MLEs
$\hat{\btheta}_{ML} = \{\hat{\beta}_{ML}, \hat{\sigma}^2_{ML}\}$
can be obtained analytically for this case by
taking the first derivatives of Equation~(\ref{eqn:log_likelihood_normal}),
setting them to zero, and
solving the equations.
The implementations for numerical solutions (from analytical solutions)
or numerical optimization
of Equation~(\ref{eqn:log_likelihood_normal}) is not difficult
and left for the reader in Exercise~\ref{ex:likelihood1}.

The assumptions of Statement~(\ref{eqn:normal}) limit the scope of modeling 
capability, and so next we introduce a more general approach.
From the independence assumption and basic multivariate statistics, 
statement~(\ref{eqn:normal})
implies a multivariate normal distribution\footnote{introduced in 
Section~\ref{sec:mvn}}\index{Distribution!multivariate normal 
distribution}\index{Distribution!MVN}
to the response variable $\by$ with dimension $N\times 1$:
\begin{equation}
\by \sim MVN_N(\bmu, \bSigma).
\label{eqn:mvn_n}
\end{equation}
where $\bmu = \bX\bbeta$ with length $N$,
$\bSigma = \sigma^2 \bI$ and $\bI$ is an $N\times N$ identity matrix.
In this case, $\by$ has density function
\begin{equation*}
\displaystyle
\phi_N(\by; \bmu, \bSigma) =
(2\pi)^{-\frac{N}{2}} |\bSigma|^{-\frac{1}{2}}
e^{-\frac{1}{2} (\by - \bmu)^\top \bSigma^{-1} (\by - \bmu)}
\end{equation*}
and the log likelihood can reduce to
Equation~(\ref{eqn:log_likelihood_normal}).
The MLEs are
$\hat{\bbeta}_{ML} = (\bX^\top \bX)^{-1} \bX^\top \by$ and
$\sigma^2_{ML} = \frac{1}{N} (\by - \bar{y}\bone)^\top(\by - \bar{y}\bone)$,
where $\bar{y}$ is the average of $\by$,
and $\bone$ is the vector of length $N$ whose entries are all 1.




\section{Likelihood Ratio Test}
\label{sec:lrt}

A very important statistical inference tool based on the likelihood 
methods discussed so far is the Likelihood Ratio Test
(LRT)\index{likelihood ratio test}\index{LRT}. 
Provided suitable assumptions hold, this test can be used to compare the fit 
of two competing models. 

Suppose we have data $\bX$ and want to test the hypothesis
\begin{align*}
  H_0: \btheta \in \bTheta_0
\end{align*}
against the alternative
\begin{align*}
  H_a: \btheta \in \bTheta_a
\end{align*}
where the two spaces $\bTheta_0$ and $\bTheta_a$ are not equivalent.
The LRT says
\begin{equation}
-2\log \Lambda(\btheta_0, \btheta_a; \bX) :=
-2\log
\frac{
\max_{\btheta\in\bTheta_0} L(\btheta; \bX)
}{
\max_{\btheta\in\bTheta_0\cup\bTheta_a} L(\btheta; \bX)
}
\sim \chi^2_p
\label{eqn:LRT}
\end{equation}
where $\btheta_0$ and $\btheta_a$ are parameters that have maximum
likelihoods in spaces $\bTheta_0$ and $\bTheta_0\cup\bTheta_a$ respectively, and
$\chi^2_p$ is a chi-squared distribution\index{Distribution!chi-squared}
with $p$ degrees of freedom. In some cases, $p$ is simply
the difference in dimension between $\bTheta_0\cup\bTheta_a$ and
$\bTheta_0$ (see example below).

For example,
in the least squares case of statement~(\ref{eqn:mvn_n}),
we may want to test
$$
  H_0: \bsigma^2 = 1 \mbox{\hspace{.4cm} v.s.\hspace{.4cm}}
  H_a: \bsigma^2 > 0 
$$
which means $\bTheta_0 = \{\bbeta\}$ and $\bTheta_a = \{\bbeta, \sigma^2\}$.
Note that $\bTheta_0 \subset \bTheta_a$, so
$\bTheta_0 \cup \bTheta_a = \bTheta_a$.
Given the MLEs $\hat{\btheta}_{0\,ML}$ and $\hat{\btheta}_{a\,ML}$
for the two spaces $\bTheta_0$ and $\bTheta_a$, the LRT will be
$$
  -2\log \hat{\Lambda}(\hat{\btheta}_{0\,ML}, \hat{\btheta}_{a\,ML}; \bX) :=
  -2\log
  \frac{
    L(\hat{\btheta}_{0\,ML}; \bX)
  }{
    L(\hat{\btheta}_{a\,ML}; \bX)
  }
  \sim \chi^2_1.
$$
For type I error $\alpha = 0.05$, if
the value
$$
  -2\log \hat{\Lambda}(\hat{\btheta}_{0\,ML}, \hat{\btheta}_{a\,ML}; \bX)
  > q_{\chi^2_1}(0.95) \approx 3.84
$$
where $q_{\chi^2_1}(0.95)$ is the 95\% quantile of chi-squared distribution
with 1 degree of freedom.  Then we may reject $H_0: \bsigma^2 = 1$ provided 
type I error is no greater than $0.05$ level.

Note that the LRT introduced here is not dependent on the types of
distributions, but has nested parameter space restriction and some
regular conditions of parameter space.
See \citet{Casella2001} or \citet{Ferguson1996} for more details of LRTs.




\section{Multivariate Normal Distribution}
\label{sec:mvn}

Suppose $\{\bX_1,\bX_2,\ldots,\bX_N\}$ is a random sample from
multivariate normal distribution
(MVN)\index{Distribution!multivariate normal distribution}\index{Distribution!MVN}
\begin{equation}
\bX_n \stackrel{i.i.d.}{\sim} MVN_p(\bmu, \bSigma)
\label{eqn:mvn_p}
\end{equation}
where $\btheta = \{\bmu, \bSigma\}$, $\bmu$ is a center with
dimension $p\times 1$, and $\bSigma$ is a $p\times p$ dispersion
matrix.
Then $\bX_n$ has density function
\begin{equation*}
\displaystyle
\phi_p(\bx_n; \bmu, \bSigma) =
(2\pi)^{-\frac{p}{2}} |\bSigma|^{-\frac{1}{2}}
\exp\left(-\frac{1}{2} (\bx_n - \bmu)^\top \bSigma^{-1} (\bx_n - \bmu)\right).
\end{equation*}
In general, $\bSigma$ could be an unstructured dispersion and must be positive
definite. Excepting over fitting problems,
an unstructured dispersion $\bSigma$ is desirable to fully
characterize correlation of dimensions since the estimation of
$\bSigma$ is completely supported by observed data and there is no
restriction on any coordinate of parameter space.

Let $\bx = (\bx_1^\top, \bx_2^\top, \ldots, \bx_N^\top)^\top$
be an observed data matrix with dimension $N\times p$.
The log likelihood function for $N$ observations is
\begin{equation}
\displaystyle
\log L(\bmu, \bSigma; \bx) = \sum_{n = 1}^N
-\frac{1}{2}
\left[
p\log(2\pi) + \log|\bSigma| + (\bx_n - \bmu)^\top \bSigma^{-1} (\bx_n - \bmu)
\right]
\label{eqn:log_likelihood_mvn_p}
\end{equation}
Note that if we wish to numerically compute the log likelihood found in 
Equation~\ref{eqn:log_likelihood_mvn_p}, the computing time grows as both $N$ 
and $p$ are increased.  In some cases, such as model-based clustering
in Chapter~\ref{chp:pmclust}, the total log likelihood is computed in each 
iteration for all samples and all components.

Suppose $\bmu$ and $\bSigma$ are known.
%FIXME what? , and no over- and under-flow,
We can efficiently compute the desired quantity using \pbdR:
\begin{lstlisting}[language=rr,title=R Code]
U <- chol(SIGMA)
logdet <- sum(log(abs(diag(U)))) * 2
B <- sweep(X.gbd, 2, MU) %*% backsolve(U, diag(1, p))

# The over- and under-flow need extral care after this step.
distval.gbd <- rowSums(B * B)

distval <- allreduce(sum(distval.gbd))
total.logL <- -(p * log(2 *pi) + logdet + distval) * 0.5
\end{lstlisting}
where \code{X.gbd} is a GBD row-major matrix with dimension
\code{N.gbd} by \code{p}, \code{MU} is a vector of length \code{p}, and
\code{SIGMA} is a \code{p} by \code{p} positive definite matrix.
The sample size $N$ will be the sum of \code{N.gbd} across all processors.
Note that this trick of computing log likelihood is a one-pass implementation
of \code{X.gbd}, \code{MU}, and \code{SIGMA}.
See HPSC~\citep{hpsc2011} or \citet{gvl} for more details.




\section{Exercises}
\label{sec:mle_exercise}

\begin{enumerate}[label=\thechapter-\arabic*]

\item
What is the definition of ``independent identical distributed''?

\item
What is the definition of ``probability density function''?

\item
Suppose $g(\cdot)$ is a continuous function with appropriate support.  Argue 
that $g\left(\hat{\theta}_{ML}\right)$ is still a maximum likelihood
estimator of $g(\theta)$.

\item
Derive MLEs from Equation~(\ref{eqn:log_likelihood_normal}).

\item
As in Exercise~\ref{ex:stat1}, argue that $\hat{\bbeta}_{ML}$ of
Equation~(\ref{eqn:log_likelihood_normal}) is also
an unbiased estimator of $\bbeta$.

\item
Show that:
\begin{itemize}
	\item $\hat{\sigma}^2_{ML}$ of
	Equation~(\ref{eqn:log_likelihood_normal})
	is a biased estimator of $\sigma^2$
	\item $\hat{\sigma}^2_{ML}$  is an asymptotically unbiased estimator of 
$\sigma^2$.
\end{itemize}

\item
Assume data are stored in GBD row-major matrix format. Implement an 
optimization function for
Equation~(\ref{eqn:log_likelihood_normal}), numerically optimized via
\code{optim()}\index{Code!\code{optim()}} in \proglang{R}.
Verify the results with the analytical solution.
\label{ex:likelihood1}

\item
Argue that Statement~(\ref{eqn:normal}) implies Statement~(\ref{eqn:mvn_n}),
provided appropriated assumption hold. 
% FIXME this is too vague for a real exercise

\item
Give an example of random variables $X$ and $Y$ which are each normally 
distributed, but $(X, Y)$ is not a multivariate normal distribution. 
{\color{blue} Hint: See Exercise~\ref{exercise:normality}.}

\item\label{exercise:normality}
Show that if $X$ and $Y$ independent random variables which are normally 
distributed, then $(X, Y)$ has a multivariate normal distribution.

\item
Prove Statement~(\ref{eqn:LRT}).
{\color{blue} Hint: \citet{Ferguson1996}.}

\item
Use a similar trick to that of Section~\ref{sec:mvn} to implement Principal 
Component Analysis (PCA).
\index{Principal Components Analysis|see{PCA}}\index{PCA}
{\color{blue} Hint: HPSC~\citep{hpsc2011}.}


\end{enumerate}

