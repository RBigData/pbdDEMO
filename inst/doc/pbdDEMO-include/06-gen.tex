%%% ----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Constructing Distributed Matrices}


{\it
``I never make exceptions. An exception disproves the rule.'' \\
\--- Sherlock Holmes
}
\vspace{0.5cm}


The \pkg{pbdBASE} and \pkg{pbdDMAT} packages offer a distributed matrix class,
\code{ddmatrix},\index{Class!\code{ddmatrix}}
as well as a collection of high-level methods for performing common matrix 
operations.  For example, if you want to compute the mean of an \proglang{R} 
matrix \code{x}, you would call 
\begin{lstlisting}[language=rr]
mean(x)
\end{lstlisting}
That's exactly the same command you would issue if \code{x} is no longer an 
ordinary \proglang{R} matrix, but a distributed matrix.  These methods range 
from simple, embarrassingly parallel operations like sums and means, to tightly 
coupled linear algebra operations like matrix-matrix multiply and singular value 
decomposition.

Unfortunately, these higher methods come with a different cost:  getting the 
data into the right format, namely the distributed matrix data structure DMAT, 
discussed at length in the previous chapter.  That said, once the hurdle of 
getting the data into the ``right format'' is out of the way, these methods 
offer very simple syntax (designed to mimic \proglang{R} as closely as 
possible), with the ability to scale computations on very large distributed 
machines.  But to get to the fun stuff, the process of exactly how to decompose 
data into a block-cyclic distribution must be addressed.  We begin dealing with 
this issue in the simplest way possible.  




\section{Fixed Global Dimension}

In these examples, we will examine the case where you know ahead of time what 
the global number of rows and columns are.

\subsection{Constructing Simple Distributed Matrices}

It is possible to construct fairly simple distributed matrices much in the same 
way that one can construct simple matrices in \proglang{R}.  We can do this 
using the functions
\code{ddmatrix()}\index{Code!\code{ddmatrix()}} and
\code{as.ddmatrix()}.\index{Code!\code{as.ddmatrix()}}
The former essentially behaves identically to \proglang{R}'s own \code{matrix()} 
function.  This function takes a global input vector/matrix \code{data=}, as 
well as the global number of rows \code{nrow=} and the global number of columns 
\code{ncol=}.  Additionally, the user may specify the blocking factor 
\code{bldim=} and the BLACS~\index{Library!BLACS} context \code{CTXT}, and the 
return is a distributed matrix.  For instance, we can specify
\begin{lstlisting}[language=rr,title=ddmatrix()]
dx <- ddmatrix(data=0, nrow=10, ncol=10)
\end{lstlisting}
to get a distributed matrix with \emph{global} dimension $10\times 10$ 
consisting of zeros.  We can also do cute things like
\begin{lstlisting}[language=rr,title=ddmatrix()]
dx <- ddmatrix(data=1:3, nrow=5, ncol=5)
\end{lstlisting}
which will create the distributed analogue of
\begin{verbatim}
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    3    2    1    3
[2,]    2    1    3    2    1
[3,]    3    2    1    3    2
[4,]    1    3    2    1    3
[5,]    2    1    3    2    1
\end{verbatim}
How exactly that ``distributed analogue'' will look (locally) depends on the 
processor grid shape (whence too, the number of processors) as well as the 
blocking factor.  This operation performs no communication.

While this can be useful, it is far from the only way to construct distributed 
matrices.  One can also convert a global (non-distributed) matrix into a 
distributed matrix.  There are some caveats; this matrix must either be owned in 
total by all processors (which is very useful in testing, but should not be used 
at scale), or the matrix is owned in total by one processor, with all others 
owning \code{NULL} for that object.  

For example, we can create identical return to the above via
\begin{lstlisting}[language=rr,title=as.ddmatrix()]
x <- matrix(data=1:3, nrow=5, ncol=5)
dx <- as.ddmatrix(x)
\end{lstlisting}
or
\begin{lstlisting}[language=rr,title=as.ddmatrix()]
if (comm.rank()==0){
  x <- matrix(data=1:3, nrow=5, ncol=5)
} else {
  x <- NULL
}

dx <- as.ddmatrix(x)
\end{lstlisting}
Each of these operations performs communication.

Other, more general combinations are possible through other means, but they are 
much more cumbersome.




\subsection{Diagonal Distributed Matrices}\label{sub:diag}

\emph{Example:  construct \textbf{diagonal} distributed matrices of specified
global dimension.}

The demo command is
\begin{Command}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(randmat_diag_global,'pbdDEMO',ask=F,echo=F)"
\end{Command}

In \proglang{R}, the \code{diag()} function serves two purposes; namely, it is 
both a reduction operation and a reverse-reduction operation, depending on the 
input.  More specifically, if given a matrix, it produces a vector containing 
the diagonal entries of that matrix; but if given a vector, it constructs a 
diagonal matrix whose diagonal is that vector.  And so for example, the zero and 
identity matrices of any dimension can quickly be constructed via:
\begin{lstlisting}[language=rr,title=Diagonal Matrices in R]
diag(x=0, nrow=10, ncol=10) # zero matrix
diag(x=1, nrow=10, ncol=10) # identity matrix
\end{lstlisting}

Both of the above functionalities of \code{diag()} are available for distributed 
matrices; however we will only focus on the latter. 

When you wish to construct a diagonal distributed matrix, you can easily do so 
by using the additional \code{type=} argument to our \code{diag()} method.  By 
default, \code{type="matrix"}, though the user may specify 
\code{type="ddmatrix"}.  If so, then as one might expect, the optional 
\code{bldim=} and \code{ICTXT=} arguments are available.  So with just a little 
bit of tweaking, the above example becomes:
\begin{lstlisting}[language=rr,title=Diagonal Matrices in pbdR]
diag(x=0, nrow=10, ncol=10, type="ddmatrix") # zero (distributed) matrix
diag(x=1, nrow=10, ncol=10, type="ddmatrix") # identity (distributed) matrix
\end{lstlisting}
In fact, the \code{type=} argument employs partial matching, so if we really 
want to be lazy, then we could simply do the following:
\begin{lstlisting}[language=rr,title=Diagonal Matrices in pbdR]
diag(x=0, nrow=10, ncol=10, type="d") # zero (distributed) matrix
diag(x=1, nrow=10, ncol=10, type="d") # identity (distributed) matrix
\end{lstlisting}

Beyond the above brief explanation, the demo for this functionality is mostly 
self-contained, although we do employ the \code{redistribute()} function to 
fully show off local data storage.  This function is explained in detail in 
Chapter~\ref{sec:redist}.






\subsection{Random Matrices}\label{subsec:rng.gl}

\emph{Example:  randomly generate distributed matrices with random normal data 
of specificed global dimension.}

The demo command is
\begin{Command}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(randmat_global,'pbdDEMO',ask=F,echo=F)"
\end{Command}

This demo shows 3 separate ways that one can generate a random normal matrix 
with specified global dimension.  The first two generate the matrix in full on 
at least one processor and distribute(s) the data, while the last method 
generates locally only what is needed.  As such, the first two can be considered 
demonstrations with what to do when you have data read in on one processor and 
need to distribute it out to the remaining processors, but for the purposes of 
building a randomly generated distributed matrix, they are not particularly 
efficient strategies.

As described in the previous section, if we have a matrix \code{x} stored on 
processor 0 and \code{NULL} on the others, then we can distribute it out as an 
object of
class \code{ddmatrix}\index{Class!\code{ddmatrix}} via the command
\code{as.ddmatrix()}.\index{Code!\code{as.ddmatrix()}}  For example
\begin{lstlisting}[language=rr]
if (comm.rank()==0){
  x <- matrix(rnorm(100), nrow=10, ncol=10)
} else {
  x <- NULL
}

dx <- as.ddmatrix(x)
\end{lstlisting}

will distribute the required data to the remaining processors.  We note for 
clarity that this is not equivalent to sending the full matrix to all processors 
and then throwing away all but what is needed.  Only the required data is 
communicated to the processors.

That said, having all of the data on all processors can be convenient while 
testing, if only for being more minimalistic in the amount of code/thinking 
required.  To do this, one need only do the following:

\begin{lstlisting}[language=rr]
x <- matrix(rnorm(100), nrow=10, ncol=10)

dx <- as.ddmatrix(x)
\end{lstlisting}

Here, each processor generates the full, global matrix, then throws away what is 
not needed.  Again, this is not efficient, but the code is concise, so it is 
extremely useful in testing.  Now, this assumes you are using the same seed on 
each processor.  This can be managed using the \pkg{pbdMPI} function 
\code{comm.set.seed()}, as in the demo script.  For more information, see that 
package's documentation.

Finally, you can generate locally only what you need.  The demo script does this 
via the \pkg{pbdDMAT} package's \code{ddmatrix()} function.  This is ``new'' 
behavior for this syntax (if you view \code{ddmatrix()} as an extension of 
\code{matrix()}).  Ordinarily you would merely execute something like
\begin{lstlisting}[language=rr,title=Creating a random normal matrix in serial 
R]
x <- rnorm(n*p)
x <- matrix(x, nrow=n, ncol=p) # this creates a copy

y <- rnorm(n*p)
dim(y) <- c(n, p) # this does not
\end{lstlisting}
However, things are slightly more complicated with \code{ddmatrix} objects, and 
the user may not easily know ahead of time what the size of the local piece is 
just from knowing the global dimension.  Because this requires a much stronger 
working knowledge of the underlying data structure than most will be comfortable 
with, we provide this simple functionality as an extension.  However, we note 
that the disciplined reader is more than capable of figuring out how it 
functions by examining the source code and checking with the reference manual.
% 
% The concise explanation is that the \code{base.numroc()} utility determines 
the size of the local storage.  This is all very well documented in the 
\pkg{pbdBASE} documentation, but since no one even pretends to read that stuff, 
\texttt{NUMROC} is a ScaLAPACK tool, which means ``\texttt{NUM}ber of 
\texttt{R}ows \texttt{O}r \texttt{C}olumns.''  The function \code{base.numroc()} 
is an implementation in \proglang{R} which calculates the number of rows 
\emph{and} columns at the same time (so it is a bit of a misnomer, but preserved 
for historical reasons).  
% 
% More precisely, it calculates the local storage requirements given a global 
dimension \code{dim}, a blocking factor \code{bldim}, and a BLACS context number 
\code{ICTXT}.  The extra argument \code{fixme} determines whether or not the 
lowest value returned should be 1.  If \code{fixme==FALSE} and any of the 
returned local dimensions are less than 1, then that processor does not actually 
own any of the global matrix --- it has no local storage.  But something must be 
stored, and so we default this to \code{matrix(0)}, the $1\times 1$ matrix with 
single entry 0.





\section{Fixed Local Dimension}

\emph{Example:  randomly generate distributed matrices with random normal data 
of specificed local dimension.}

The demo command is
\begin{Command}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(randmat_local,'pbdDEMO',ask=F,echo=F)"
\end{Command}

This is similar to the above, but with a critical difference.  Instead of 
specifying a fixed \emph{global} dimension and then go determine what the local 
storage space is, instead we specify a fixed \emph{local} dimension and then go 
figure out what the global dimension should be.  This can be useful for testing 
weak scaling of an algorithm, where different numbers of cores are used with the 
same local problem size.

To this end, the demo script utilizes the \code{ddmatrix.local()} function, 
which has the user specify a local dimension size that all the processors should 
use, as well as a blocking factor and BLACS~\index{Library!BLACS} context.  Now 
here things get somewhat tricky, because in order for this matrix to exist at 
all, each margin of the blocking factor must divide (as an integer) the 
corresponding margin of the global dimension.  To better understand why this is 
so, the reader is suggested to read the \pkg{pbdDMAT} vignette.  But if you 
already understand or are merely willing to take it on faith, then you surely 
grant that this is a problem.

So here, we assume that the local dimension is chosen appropriately by the user, 
but it is possible that a bad blocking factor is supplied by the user.  Rather 
than halt with a stop error, we attempt to find the next best blocking factor 
possible.  To do this, we must find the smallest integer above the specified 
blocking factor that will divide the number of local rows or columns.




\section{Exercises}
\label{sec:statistics_exercise}

\begin{enumerate}[label=\thechapter-\arabic*]
\item
Random number generation (RNG) is used in this Section such as \code{rnorm()}.
In \proglang{pbdR}, we use an \proglang{R} package
\pkg{rlecuyer}~\citep{rlecuyer}\index{Package!\pkg{rlecuyer}}
to set different streams of seed in
parallel. Try to find and use other RNG methods or implementations
in \proglang{R}.

\end{enumerate}
